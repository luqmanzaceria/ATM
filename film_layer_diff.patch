diff --git a/atm/model/film.py b/atm/model/film.py
new file mode 100644
index 0000000..343d7d7
--- /dev/null
+++ b/atm/model/film.py
@@ -0,0 +1,13 @@
+# atm/model/film.py
+import torch
+import torch.nn as nn
+
+class FiLMLayer(nn.Module):
+    def __init__(self, input_dim, condition_dim):
+        super().__init__()
+        self.condition_network = nn.Linear(condition_dim, input_dim * 2)
+
+    def forward(self, x, condition):
+        film_params = self.condition_network(condition)
+        gamma, beta = torch.chunk(film_params, 2, dim=-1)
+        return (1 + gamma) * x + beta
\ No newline at end of file
diff --git a/atm/model/track_transformer.py b/atm/model/track_transformer.py
index 644a445..e49c765 100644
--- a/atm/model/track_transformer.py
+++ b/atm/model/track_transformer.py
@@ -12,6 +12,8 @@ from atm.policy.vilt_modules.language_modules import *
 from .track_patch_embed import TrackPatchEmbed
 from .transformer import Transformer
 
+from atm.model.film import FiLMLayer
+
 class TrackTransformer(nn.Module):
     """
     flow video model using a BERT transformer
@@ -38,6 +40,8 @@ class TrackTransformer(nn.Module):
         self.language_encoder = self._init_language_encoder(output_size=dim, **language_encoder_cfg)
         self._init_weights(self.dim, self.num_img_patches)
 
+        self.film_layer = FiLMLayer(dim, dim)
+
         print(f"TrackTransformer: Image size: {self.img_size}")
         print(f"TrackTransformer: Patch size: {self.img_proj_encoder.patch_size}")
         print(f"TrackTransformer: Number of patches: {self.num_img_patches}")
@@ -189,17 +193,21 @@ class TrackTransformer(nn.Module):
     
         text_encoded = self.language_encoder(task_emb)  # (b, c)
         text_encoded = rearrange(text_encoded, 'b c -> b 1 c')
-    
-        x = torch.cat([enc_track, patches, text_encoded], dim=1)
+        print(f"Language token shape: {text_encoded.shape}")
+
+        x = torch.cat([enc_track, patches], dim=1)
+        x = self.film_layer(x, text_encoded)
         print(f"Shape before transformer: {x.shape}")
-        x = self.transformer(x)
+        x = self.transformer(x, language_condition=text_encoded)  # Pass language_condition here
         print(f"Shape after transformer: {x.shape}")
     
         # Extract the CLS token
         cls_token = x[:, 0]
+        print(f"CLS token shape: {cls_token.shape}")
     
         # Get the track representation
         track_rep = x[:, 1:self.num_track_patches+1]
+        print(f"Track representation shape: {track_rep.shape}")
     
         rec_patches = self.img_decoder(x[:, self.num_track_patches+1:-1])
         print(f"rec_patches shape: {rec_patches.shape}")
@@ -358,4 +366,17 @@ class TrackTransformer(nn.Module):
         torch.save(self.state_dict(), path)
 
     def load(self, path):
-        self.load_state_dict(torch.load(path, map_location="cpu"))
+        state_dict = torch.load(path, map_location="cpu")
+        model_dict = self.state_dict()
+        
+        # Filter out unnecessary keys
+        pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}
+        
+        # Update model state dict
+        model_dict.update(pretrained_dict)
+        
+        # Load the filtered state dict
+        self.load_state_dict(model_dict, strict=False)
+        
+        print(f"Loaded model from {path}")
+        print(f"Newly initialized layers: {set(model_dict.keys()) - set(pretrained_dict.keys())}")
diff --git a/atm/model/transformer.py b/atm/model/transformer.py
index 626ae14..3c8b21c 100644
--- a/atm/model/transformer.py
+++ b/atm/model/transformer.py
@@ -4,6 +4,8 @@ from torch import nn, einsum
 
 from einops import rearrange
 
+from atm.model.film import FiLMLayer
+
 def exists(val):
     return val is not None
 
@@ -109,7 +111,7 @@ class TransformerAttention(nn.Module):
         return self.to_out(out)
 
 class Transformer(nn.Module):
-    def __init__(self, dim, dim_head=64, heads=8, depth=6, attn_dropout=0., ff_dropout=0.):
+    def __init__(self, dim, dim_head=64, heads=8, depth=6, attn_dropout=0., ff_dropout=0., use_film=True):
         super().__init__()
         self.layers = nn.ModuleList([])
         for _ in range(depth):
@@ -119,14 +121,22 @@ class Transformer(nn.Module):
                         TransformerAttention(dim=dim, heads=heads, dropout=attn_dropout, dim_head=dim_head),
                         FeedForward(dim=dim, dropout=ff_dropout)
                     ]))
+        self.use_film = use_film
+        if self.use_film:
+            self.film_layers = nn.ModuleList([FiLMLayer(dim, dim) for _ in range(depth)])
+
+    def forward(self, x, language_condition=None, cond_fns=None, attn_mask=None):
+        if self.use_film:
+            assert exists(language_condition), "language_condition must be provided when use_film is True"
 
-    def forward(self, x, cond_fns=None, attn_mask=None):
         if not exists(cond_fns):
             cond_fns = (None,) * len(self.layers) * 2
 
         cond_fns = iter(cond_fns)
 
-        for attn, ff in self.layers:
+        for i, (attn, ff) in enumerate(self.layers):
             x = attn(x, attn_mask=attn_mask, cond_fn=next(cond_fns)) + x
             x = ff(x, cond_fn=next(cond_fns)) + x
+            if self.use_film:
+                x = self.film_layers[i](x, language_condition)
         return x
\ No newline at end of file
diff --git a/atm/policy/vilt.py b/atm/policy/vilt.py
index 8062df8..0cf459a 100644
--- a/atm/policy/vilt.py
+++ b/atm/policy/vilt.py
@@ -17,6 +17,8 @@ from atm.policy.vilt_modules.extra_state_modules import ExtraModalityTokens
 from atm.policy.vilt_modules.policy_head import *
 from atm.utils.flow_utils import ImageUnNormalize, sample_double_grid, tracks_to_video
 
+from atm.model.track_transformer import FiLMLayer
+
 ###############################################################################
 #
 # A ViLT Policy
@@ -66,8 +68,10 @@ class BCViLTPolicy(nn.Module):
 
         self.track_proj1 = nn.Linear(49536, self.spatial_embed_size)  # Adjust input size as needed
         self.track_proj2 = nn.Linear(256, self.spatial_embed_size)
-    
 
+        self.film_layer = FiLMLayer(self.spatial_embed_size, self.spatial_embed_size)
+        self.language_proj_spatial = nn.Linear(self.spatial_embed_size * 2, self.spatial_embed_size)
+    
         if load_path is not None:
             self.load(load_path)
             self.track.load(f"{track_cfg.track_fn}/model_best.ckpt")
@@ -98,7 +102,13 @@ class BCViLTPolicy(nn.Module):
         self.img_num_patches = sum([x.num_patches for x in self.image_encoders])
 
     def _setup_language_encoder(self, network_name, **language_encoder_kwargs):
-        return eval(network_name)(**language_encoder_kwargs)
+        language_output_size = self.spatial_embed_size * 2  # Double the size
+        
+        # Remove 'output_size' from kwargs if it exists
+        language_encoder_kwargs.pop('output_size', None)
+        
+        # Now pass output_size explicitly
+        return eval(network_name)(output_size=language_output_size, **language_encoder_kwargs)
 
     def _setup_track(self, track_fn, policy_track_patch_size=None, use_zero_track=False):
         """
@@ -295,7 +305,8 @@ class BCViLTPolicy(nn.Module):
         B, T = img_encoded.shape[:2]
     
         # 2. encode task_emb
-        text_encoded = self.language_encoder_spatial(task_emb)  # (b, c)
+        text_encoded = self.language_encoder_spatial(task_emb)  # (b, c*2)
+        text_encoded = self.language_proj_spatial(text_encoded)  # (b, c)
         text_encoded = text_encoded.view(B, 1, 1, -1).expand(-1, T, -1, -1)  # (b, t, 1, c)
     
         # 3. encode track
@@ -322,14 +333,25 @@ class BCViLTPolicy(nn.Module):
         track_encoded = rearrange(track_encoded, 'b t v c -> b t (v c)')
         track_encoded = self.track_proj2(track_encoded)
         print(f"final track_encoded shape: {track_encoded.shape}")
-    
+        
+        # Concatenate image and track embeddings
+        img_track_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2)], -2)  # (b, t, num_img_patch + 1, c)
+        
+        # Expand text_encoded to match the time dimension of img_track_encoded
+        text_encoded_expanded = text_encoded.expand(-1, img_track_encoded.shape[1], -1, -1)
+        
+        # Apply FiLM layer
+        img_track_encoded = self.film_layer(img_track_encoded, text_encoded_expanded)
+        
         # 3. concat img + track + text embs then add modality embeddings
         if self.spatial_transformer_use_text:
-            img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2), text_encoded], -2)  # (b, t, 2*num_img_patch + 1 + 1, c)
+            img_track_text_encoded = torch.cat([img_track_encoded, text_encoded], -2)  # (b, t, num_img_patch + 1 + 1, c)
             img_track_text_encoded += self.modality_embed[None, :, self.modality_idx, :]
         else:
-            img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2)], -2)  # (b, t, 2*num_img_patch + 1, c)
+            img_track_text_encoded = img_track_encoded  # (b, t, num_img_patch + 1, c)
             img_track_text_encoded += self.modality_embed[None, :, self.modality_idx[:-1], :]
+        
+        print(f"img_track_text_encoded shape: {img_track_text_encoded.shape}")
     
         # 4. add spatial token
         spatial_token = self.spatial_token.unsqueeze(0).expand(B, T, -1, -1)  # (b, t, 1, c)
diff --git a/shared_mlp_diff.patch b/shared_mlp_diff.patch
new file mode 100644
index 0000000..cc2009f
--- /dev/null
+++ b/shared_mlp_diff.patch
@@ -0,0 +1,100 @@
+diff --git a/atm/policy/vilt.py b/atm/policy/vilt.py
+index 8062df8..ed9fed3 100644
+--- a/atm/policy/vilt.py
++++ b/atm/policy/vilt.py
+@@ -64,8 +64,15 @@ class BCViLTPolicy(nn.Module):
+         # 8. define policy head
+         self._setup_policy_head(**policy_head_cfg)
+ 
+-        self.track_proj1 = nn.Linear(49536, self.spatial_embed_size)  # Adjust input size as needed
+-        self.track_proj2 = nn.Linear(256, self.spatial_embed_size)
++        # self.track_proj1 = nn.Linear(49536, self.spatial_embed_size)  # Adjust input size as needed
++        # self.track_proj2 = nn.Linear(256, self.spatial_embed_size)
++
++        # Define the shared MLP to handle the correct input size
++        self.shared_mlp = nn.Sequential(
++            nn.Linear(49536, 128),  # Adjusted to match the input size of the track features
++            nn.ReLU(),
++            nn.Linear(128, self.spatial_embed_size)
++        )
+     
+ 
+         if load_path is not None:
+@@ -299,31 +306,17 @@ class BCViLTPolicy(nn.Module):
+         text_encoded = text_encoded.view(B, 1, 1, -1).expand(-1, T, -1, -1)  # (b, t, 1, c)
+     
+         # 3. encode track
+-        track_encoded = self._encode_track(track_obs, task_emb)  # track_encoded: (b, v, t, cls_dim + track_rep_dim)
+-        print(f"track_encoded shape: {track_encoded.shape}")
+-        
+-        track_encoded = rearrange(track_encoded, 'b v t c -> (b t) v c')
+-        print(f"track_encoded shape after rearrange: {track_encoded.shape}")
+-        
+-        # Project track_encoded to match the embedding size
+-        track_encoded = self.track_proj1(track_encoded)
+-        print(f"track_encoded shape after projection: {track_encoded.shape}")
++        track_encoded = self._encode_track(track_obs, task_emb)  # Shape: (b, v, t, 49536)
++        track_encoded = rearrange(track_encoded, 'b v t c -> (b v t) c')  # Flatten for shared MLP application
+         
+-        # Reshape back to (b, t, v, c)
+-        track_encoded = rearrange(track_encoded, '(b t) v c -> b t v c', b=B, t=T)
+-        
+-        # Adjust track_pos_embed to match the dimensions of track_encoded
+-        track_pos_embed = self.track_pos_embed.expand(B, T, self.num_views, -1)
+-        print(f"track_pos_embed shape: {track_pos_embed.shape}")
+-        
+-        track_encoded += track_pos_embed  # (b, t, v, c)
+-        
+-        # Flatten the view dimension
+-        track_encoded = rearrange(track_encoded, 'b t v c -> b t (v c)')
+-        track_encoded = self.track_proj2(track_encoded)
+-        print(f"final track_encoded shape: {track_encoded.shape}")
++        # Apply shared MLP to each track representation
++        track_encoded = self.shared_mlp(track_encoded)  # Apply shared MLP, Shape after MLP: (b*v*t, spatial_embed_size)
++        track_encoded = rearrange(track_encoded, '(b v t) c -> b t v c', b=B, v=self.num_views, t=T)  # Reshape back to (b, t, v, c)
+     
+-        # 3. concat img + track + text embs then add modality embeddings
++        # Max pooling across views
++        track_encoded, _ = torch.max(track_encoded, dim=2)  # Max pooling across views, Shape: (b, t, c)
++    
++        # 4. concat img + track + text embs then add modality embeddings
+         if self.spatial_transformer_use_text:
+             img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2), text_encoded], -2)  # (b, t, 2*num_img_patch + 1 + 1, c)
+             img_track_text_encoded += self.modality_embed[None, :, self.modality_idx, :]
+@@ -331,23 +324,23 @@ class BCViLTPolicy(nn.Module):
+             img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2)], -2)  # (b, t, 2*num_img_patch + 1, c)
+             img_track_text_encoded += self.modality_embed[None, :, self.modality_idx[:-1], :]
+     
+-        # 4. add spatial token
++        # 5. add spatial token
+         spatial_token = self.spatial_token.unsqueeze(0).expand(B, T, -1, -1)  # (b, t, 1, c)
+         encoded = torch.cat([spatial_token, img_track_text_encoded], -2)  # (b, t, 1 + 2*num_img_patch + 1 + 1, c)
+     
+-        # 5. pass through transformer
+-        encoded = rearrange(encoded, "b t n c -> (b t) n c")  # (b*t, 1 + 2*num_img_patch + v + 1, c)
++        # 6. pass through transformer
++        encoded = rearrange(encoded, "b t n c -> (b t) n c")  # (b*t, 1 + 2*num_img_patch + 1 + 1, c)
+         out = self.spatial_transformer(encoded)
+         out = out[:, 0]  # extract spatial token as summary at o_t
+         out = self.spatial_downsample(out).view(B, T, 1, -1)  # (b, t, 1, c')
+     
+-        # 6. encode extra states
++        # 7. encode extra states
+         if self.extra_encoder is None:
+             extra = None
+         else:
+             extra = self.extra_encoder(extra_states)  # (B, T, num_extra, c')
+     
+-        # 7. encode language, treat it as action token
++        # 8. encode language, treat it as action token
+         text_encoded_ = self.language_encoder_temporal(task_emb)  # (b, c')
+         text_encoded_ = text_encoded_.view(B, 1, 1, -1).expand(-1, T, -1, -1)  # (b, t, 1, c')
+         action_cls_token = self.action_cls_token.unsqueeze(0).expand(B, T, -1, -1)  # (b, t, 1, c')
+@@ -365,6 +358,7 @@ class BCViLTPolicy(nn.Module):
+     
+         return output
+ 
++
+     def temporal_encode(self, x):
+         """
+         Args:
