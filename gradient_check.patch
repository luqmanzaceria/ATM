diff --git a/atm/dataloader/bc_dataloader.py b/atm/dataloader/bc_dataloader.py
index 19c4bc7..a6d11a6 100644
--- a/atm/dataloader/bc_dataloader.py
+++ b/atm/dataloader/bc_dataloader.py
@@ -9,13 +9,27 @@ class BCDataset(BaseDataset):
     def __init__(self, track_obs_fs=1, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.track_obs_fs = track_obs_fs
+        self.task_names = self._get_task_names()
+        self.task_ids = {name: i for i, name in enumerate(self.task_names)}
+
+    def _get_task_names(self):
+        # Extract task names from the dataset
+        task_names = set()
+        for demo_path in self._demo_id_to_path.values():
+            parts = demo_path.split('/')
+            if 'libero_spatial' in parts:
+                idx = parts.index('libero_spatial')
+                if idx + 1 < len(parts):
+                    task_names.add(parts[idx + 1])
+        return sorted(task_names)
 
     def __getitem__(self, index):
         demo_id = self._index_to_demo_id[index]
         demo_start_index = self._demo_id_to_start_indices[demo_id]
-
+        demo_path = self._demo_id_to_path[demo_id]
+    
         time_offset = index - demo_start_index
-
+    
         if self.cache_all:
             demo = self._cache[demo_id]
             all_view_frames = []
@@ -41,7 +55,7 @@ class BCDataset(BaseDataset):
                 all_view_track_transformer_frames.append(
                     torch.stack([self._load_image_list_from_demo(demo, view, time_offset + t, num_frames=self.track_obs_fs, backward=True) for t in range(self.frame_stack)])
                 )  # t tt_fs c h w
-
+    
         all_view_tracks = []
         all_view_vis = []
         for view in self.views:
@@ -52,17 +66,17 @@ class BCDataset(BaseDataset):
                 all_time_step_vis.append(demo["root"][view]['vis'][track_start_index:track_start_index + self.num_track_ts])  # track_len n
             all_view_tracks.append(torch.stack(all_time_step_tracks, dim=0))
             all_view_vis.append(torch.stack(all_time_step_vis, dim=0))
-
+    
         obs = torch.stack(all_view_frames, dim=0)  # v t c h w
         track = torch.stack(all_view_tracks, dim=0)  # v t track_len n 2
         vi = torch.stack(all_view_vis, dim=0)  # v t track_len n
         track_transformer_obs = torch.stack(all_view_track_transformer_frames, dim=0)  # v t tt_fs c h w
-
+    
         # augment rgbs and tracks
         if np.random.rand() < self.aug_prob:
             obs, track = self.augmentor((obs / 255., track))
             obs = obs * 255.
-
+    
         # sample tracks
         sample_track, sample_vi = [], []
         for i in range(len(self.views)):
@@ -75,10 +89,16 @@ class BCDataset(BaseDataset):
             sample_vi.append(torch.stack(sample_vi_per_time, dim=0))
         track = torch.stack(sample_track, dim=0)
         vi = torch.stack(sample_vi, dim=0)
-
+    
         actions = demo["root"]["actions"][time_offset:time_offset + self.frame_stack]
         task_embs = demo["root"]["task_emb_bert"]
         extra_states = {k: v[time_offset:time_offset + self.frame_stack] for k, v in
                         demo['root']['extra_states'].items()}
-
-        return obs, track_transformer_obs, track, task_embs, actions, extra_states
+    
+        # Get the task name and ID
+        parts = demo_path.split('/')
+        idx = parts.index('libero_spatial')
+        task_name = parts[idx + 1] if idx + 1 < len(parts) else "unknown"
+        task_id = self.task_ids.get(task_name, -1)  # Use -1 for unknown tasks
+    
+        return obs, track_transformer_obs, track, task_embs, actions, extra_states, task_id
\ No newline at end of file
diff --git a/atm/dataloader/utils.py b/atm/dataloader/utils.py
index 3ac46f1..691c837 100644
--- a/atm/dataloader/utils.py
+++ b/atm/dataloader/utils.py
@@ -1,6 +1,7 @@
 import numpy as np
 from PIL import Image
 from einops import repeat
+import torch
 import torchvision
 from torch.utils.data import DataLoader
 import torch.nn as nn
@@ -11,16 +12,32 @@ import robomimic.utils.tensor_utils as TensorUtils
 from robomimic.models.obs_core import CropRandomizer
 
 
-def get_dataloader(replay, mode, num_workers, batch_size):
-    loader = DataLoader(
-        replay,
-        shuffle=(mode == "train"),
-        pin_memory=True,
+def custom_collate_fn(batch):
+    # Separate the task_ids from the rest of the data
+    obs, track_transformer_obs, track, task_embs, actions, extra_states, task_ids = zip(*batch)
+    
+    # Collate the data
+    collated_data = (
+        torch.stack(obs),
+        torch.stack(track_transformer_obs),
+        torch.stack(track),
+        torch.stack(task_embs),
+        torch.stack(actions),
+        {k: torch.stack([d[k] for d in extra_states]) for k in extra_states[0]},
+        torch.tensor(task_ids)
+    )
+    return collated_data
+
+def get_dataloader(dataset, mode="train", num_workers=4, batch_size=32):
+    return DataLoader(
+        dataset,
         batch_size=batch_size,
+        shuffle=(mode == "train"),
         num_workers=num_workers,
+        collate_fn=custom_collate_fn,
+        pin_memory=True,
         prefetch_factor=4 if num_workers > 0 else None
     )
-    return loader
 
 
 def load_rgb(file_name):
diff --git a/atm/model/track_patch_embed.py b/atm/model/track_patch_embed.py
index 92864d0..4dfca34 100644
--- a/atm/model/track_patch_embed.py
+++ b/atm/model/track_patch_embed.py
@@ -2,6 +2,41 @@ import torch
 from torch import nn
 from einops import rearrange
 
+class TrackPatchEmbed(nn.Module):
+    def __init__(self,
+                 num_track_ts,
+                 num_track_ids,
+                 patch_size,
+                 in_dim,
+                 embed_dim):
+        super().__init__()
+        self.num_track_ts = num_track_ts
+        self.num_track_ids = num_track_ids
+        self.patch_size = patch_size
+        self.in_dim = in_dim
+        self.embed_dim = embed_dim
+
+        assert self.num_track_ts % self.patch_size == 0, "num_track_ts must be divisible by patch_size"
+        self.num_patches_per_track = self.num_track_ts // self.patch_size
+        self.num_patches = self.num_patches_per_track * self.num_track_ids
+
+        self.conv = nn.Conv1d(in_dim, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)
+
+    def forward(self, tracks):
+        """
+        tracks: (B, T, N, in_dim)
+
+        embed the tracks into patches. make sure to reshape into (B, N, T, out_dim) at the end
+        """
+        b, t, n, c = tracks.shape
+        tracks = rearrange(tracks, 'b t n c -> (b n) c t')
+        patches = self.conv(tracks)
+        patches = rearrange(patches, '(b n) c t -> b t n c', b=b, n=n)
+
+        return patchesimport torch
+from torch import nn
+from einops import rearrange
+
 class TrackPatchEmbed(nn.Module):
     def __init__(self,
                  num_track_ts,
diff --git a/atm/model/track_transformer.py b/atm/model/track_transformer.py
index 644a445..120bace 100644
--- a/atm/model/track_transformer.py
+++ b/atm/model/track_transformer.py
@@ -38,15 +38,15 @@ class TrackTransformer(nn.Module):
         self.language_encoder = self._init_language_encoder(output_size=dim, **language_encoder_cfg)
         self._init_weights(self.dim, self.num_img_patches)
 
-        print(f"TrackTransformer: Image size: {self.img_size}")
-        print(f"TrackTransformer: Patch size: {self.img_proj_encoder.patch_size}")
-        print(f"TrackTransformer: Number of patches: {self.num_img_patches}")
-        print(f"TrackTransformer: Transformer dim: {self.dim}")
-        print(f"TrackTransformer: Number of track patches: {self.num_track_patches}")
+        # print(f"TrackTransformer: Image size: {self.img_size}")
+        # print(f"TrackTransformer: Patch size: {self.img_proj_encoder.patch_size}")
+        # print(f"TrackTransformer: Number of patches: {self.num_img_patches}")
+        # print(f"TrackTransformer: Transformer dim: {self.dim}")
+        # print(f"TrackTransformer: Number of track patches: {self.num_track_patches}")
 
         if load_path is not None:
             self.load(load_path)
-            print(f"loaded model from {load_path}")
+            # print(f"loaded model from {load_path}")
             
     def _init_transformer(self, dim, dim_head, heads, depth, attn_dropout, ff_dropout):
         self.transformer = Transformer(
@@ -120,7 +120,7 @@ class TrackTransformer(nn.Module):
         img_embed = rearrange(img_embed, 'n d -> () n d')
         self.img_embed.data.copy_(torch.from_numpy(img_embed))
 
-        print(f"num_track_patches: {self.num_track_patches}, num_img_patches: {num_img_patches}, total: {self.num_track_patches + num_img_patches}")
+        # print(f"num_track_patches: {self.num_track_patches}, num_img_patches: {num_img_patches}, total: {self.num_track_patches + num_img_patches}")
 
     def _preprocess_track(self, track):
         return track
@@ -191,9 +191,9 @@ class TrackTransformer(nn.Module):
         text_encoded = rearrange(text_encoded, 'b c -> b 1 c')
     
         x = torch.cat([enc_track, patches, text_encoded], dim=1)
-        print(f"Shape before transformer: {x.shape}")
+        # print(f"Shape before transformer: {x.shape}")
         x = self.transformer(x)
-        print(f"Shape after transformer: {x.shape}")
+        # print(f"Shape after transformer: {x.shape}")
     
         # Extract the CLS token
         cls_token = x[:, 0]
@@ -202,8 +202,8 @@ class TrackTransformer(nn.Module):
         track_rep = x[:, 1:self.num_track_patches+1]
     
         rec_patches = self.img_decoder(x[:, self.num_track_patches+1:-1])
-        print(f"rec_patches shape: {rec_patches.shape}")
-        print(f"patches shape: {patches.shape}")
+        # print(f"rec_patches shape: {rec_patches.shape}")
+        # print(f"patches shape: {patches.shape}")
     
         num_track_h = self.num_track_ts // self.track_patch_size
         rec_track = self.track_decoder(track_rep)
@@ -270,8 +270,8 @@ class TrackTransformer(nn.Module):
         rec_track, rec_patches, cls_token, track_rep = self.forward(vid, track, task_emb, p_img)
     
         patchified_vid = self._patchify(vid)
-        print(f"rec_patches shape: {rec_patches.shape}")
-        print(f"patchified vid shape: {patchified_vid.shape}")
+        # print(f"rec_patches shape: {rec_patches.shape}")
+        # print(f"patchified vid shape: {patchified_vid.shape}")
         
         # Ensure the shapes match for loss calculation
         min_patches = min(rec_patches.shape[1], patchified_vid.shape[1])
@@ -282,9 +282,9 @@ class TrackTransformer(nn.Module):
         img_loss = F.mse_loss(rec_patches, patchified_vid)
         loss = track_loss + img_loss
     
-        print(f"Before _unpatchify: rec_patches shape = {rec_patches.shape}")
+        # print(f"Before _unpatchify: rec_patches shape = {rec_patches.shape}")
         rec_image = self._unpatchify(rec_patches)
-        print(f"After _unpatchify: rec_image shape = {rec_image.shape}")
+        # print(f"After _unpatchify: rec_image shape = {rec_image.shape}")
 
         # place them side by side
         combined_image = torch.cat([vid[:, -1], rec_image[:, -1]], dim=-1)  # only visualize the current frame
@@ -337,11 +337,372 @@ class TrackTransformer(nn.Module):
         h = self.img_size[0] // p
         w = self.img_size[1] // p
         
-        print(f"_unpatchify: x.shape = {x.shape}")
-        print(f"_unpatchify: Expected patches = {h * w}, Actual patches = {x.shape[1]}")
+        # print(f"_unpatchify: x.shape = {x.shape}")
+        # print(f"_unpatchify: Expected patches = {h * w}, Actual patches = {x.shape[1]}")
         
         if h * w != x.shape[1]:
-            print(f"Warning: Number of patches mismatch. Expected {h * w}, got {x.shape[1]}. Adjusting...")
+            # print(f"Warning: Number of patches mismatch. Expected {h * w}, got {x.shape[1]}. Adjusting...")
+            # Pad or trim x to match the expected number of patches
+            if h * w > x.shape[1]:
+                pad_size = h * w - x.shape[1]
+                x = F.pad(x, (0, 0, 0, pad_size))
+            else:
+                x = x[:, :h*w, :]
+    
+        x = rearrange(x, "n (h w) (p q t c) -> n h w p q t c", h=h, w=w, p=p, q=p, t=self.frame_stack, c=3)
+        x = rearrange(x, "n h w p q t c -> n t c h p w q")
+        imgs = rearrange(x, "n t c h p w q -> n t c (h p) (w q)")
+        return imgs
+
+    def save(self, path):
+        torch.save(self.state_dict(), path)
+
+    def load(self, path):
+        self.load_state_dict(torch.load(path, map_location="cpu"))
+import numpy as np
+import torch
+import torch.nn.functional as F
+import torchvision.transforms as T
+from einops import rearrange, repeat
+from timm.models.vision_transformer import PatchEmbed
+from torch import nn
+
+from atm.utils.flow_utils import ImageUnNormalize, tracks_to_video
+from atm.utils.pos_embed_utils import get_1d_sincos_pos_embed, get_2d_sincos_pos_embed
+from atm.policy.vilt_modules.language_modules import *
+from .track_patch_embed import TrackPatchEmbed
+from .transformer import Transformer
+
+class TrackTransformer(nn.Module):
+    """
+    flow video model using a BERT transformer
+
+    dim: int, dimension of the model
+    depth: int, number of layers
+    heads: int, number of heads
+    dim_head: int, dimension of each head
+    attn_dropout: float, dropout for attention layers
+    ff_dropout: float, dropout for feedforward layers
+    """
+
+    def __init__(self,
+                 transformer_cfg,
+                 track_cfg,
+                 vid_cfg,
+                 language_encoder_cfg,
+                 load_path=None):
+        super().__init__()
+        self.dim = dim = transformer_cfg.dim
+        self.transformer = self._init_transformer(**transformer_cfg)
+        self.track_proj_encoder, self.track_decoder = self._init_track_modules(**track_cfg, dim=dim)
+        self.img_proj_encoder, self.img_decoder = self._init_video_modules(**vid_cfg, dim=dim)
+        self.language_encoder = self._init_language_encoder(output_size=dim, **language_encoder_cfg)
+        self._init_weights(self.dim, self.num_img_patches)
+
+        # print(f"TrackTransformer: Image size: {self.img_size}")
+        # print(f"TrackTransformer: Patch size: {self.img_proj_encoder.patch_size}")
+        # print(f"TrackTransformer: Number of patches: {self.num_img_patches}")
+        # print(f"TrackTransformer: Transformer dim: {self.dim}")
+        # print(f"TrackTransformer: Number of track patches: {self.num_track_patches}")
+
+        if load_path is not None:
+            self.load(load_path)
+            # print(f"loaded model from {load_path}")
+            
+    def _init_transformer(self, dim, dim_head, heads, depth, attn_dropout, ff_dropout):
+        self.transformer = Transformer(
+            dim=dim,
+            dim_head=dim_head,
+            heads=heads,
+            depth=depth,
+            attn_dropout=attn_dropout,
+            ff_dropout=ff_dropout)
+
+        return self.transformer
+
+    def _init_track_modules(self, dim, num_track_ts, num_track_ids, patch_size=1):
+        self.num_track_ts = num_track_ts
+        self.num_track_ids = num_track_ids
+        self.track_patch_size = patch_size
+
+        self.track_proj_encoder = TrackPatchEmbed(
+            num_track_ts=num_track_ts,
+            num_track_ids=num_track_ids,
+            patch_size=patch_size,
+            in_dim=2,
+            embed_dim=dim)
+        self.num_track_patches = self.track_proj_encoder.num_patches
+        self.track_decoder = nn.Linear(dim, 2 * patch_size, bias=True)
+        self.num_track_ids = num_track_ids
+        self.num_track_ts = num_track_ts
+
+        return self.track_proj_encoder, self.track_decoder
+
+    def _init_video_modules(self, dim, img_size, patch_size, frame_stack=1, img_mean=[.5, .5, .5], img_std=[.5, .5, .5]):
+        self.img_normalizer = T.Normalize(img_mean, img_std)
+        self.img_unnormalizer = ImageUnNormalize(img_mean, img_std)
+        if isinstance(img_size, int):
+            img_size = (img_size, img_size)
+        else:
+            img_size = (img_size[0], img_size[1])
+        self.img_size = img_size
+        self.frame_stack = frame_stack
+        self.patch_size = patch_size
+        self.img_proj_encoder = PatchEmbed(
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=3 * self.frame_stack,
+            embed_dim=dim,
+        )
+        self.num_img_patches = self.img_proj_encoder.num_patches
+        self.img_decoder = nn.Linear(dim, 3 * self.frame_stack * patch_size ** 2, bias=True)
+
+        return self.img_proj_encoder, self.img_decoder
+
+    def _init_language_encoder(self, network_name, **language_encoder_kwargs):
+        return eval(network_name)(**language_encoder_kwargs)
+
+    def _init_weights(self, dim, num_img_patches):
+        """
+        initialize weights; freeze all positional embeddings
+        """
+        num_track_t = self.num_track_ts // self.track_patch_size
+
+        self.track_embed = nn.Parameter(torch.randn(1, num_track_t, 1, dim), requires_grad=True)
+        self.img_embed = nn.Parameter(torch.randn(1, num_img_patches, dim), requires_grad=False)
+        self.mask_token = nn.Parameter(torch.randn(1, 1, dim))
+
+        track_embed = get_1d_sincos_pos_embed(dim, num_track_t)
+        track_embed = rearrange(track_embed, 't d -> () t () d')
+        self.track_embed.data.copy_(torch.from_numpy(track_embed))
+
+        num_patches_h, num_patches_w = self.img_size[0] // self.patch_size, self.img_size[1] // self.patch_size
+        img_embed = get_2d_sincos_pos_embed(dim, (num_patches_h, num_patches_w))
+        img_embed = rearrange(img_embed, 'n d -> () n d')
+        self.img_embed.data.copy_(torch.from_numpy(img_embed))
+
+        # print(f"num_track_patches: {self.num_track_patches}, num_img_patches: {num_img_patches}, total: {self.num_track_patches + num_img_patches}")
+
+    def _preprocess_track(self, track):
+        return track
+
+    def _preprocess_vis(self, vis):
+        return vis
+
+    def _preprocess_vid(self, vid):
+        assert torch.max(vid) >= 2
+
+        vid = vid[:, -self.frame_stack:]
+        vid = self.img_normalizer(vid / 255.)
+        return vid
+
+    def _encode_track(self, track):
+        """
+        track: (b, t, n, 2)
+        """
+        b, t, n, _ = track.shape
+        track = self._mask_track_as_first(track)  # b, t, n, d. track embedding is 1, t, 1, d
+        track = self.track_proj_encoder(track)
+
+        track = track + self.track_embed
+        track = rearrange(track, 'b t n d -> b (t n) d')
+        return track
+
+    def _encode_video(self, vid, p):
+        """
+        vid: (b, t, c, h, w)
+        """
+        vid = rearrange(vid, "b t c h w -> b (t c) h w")
+        patches = self.img_proj_encoder(vid)  # b, n, d
+        patches = self._mask_patches(patches, p=p)
+        patches = patches + self.img_embed
+
+        return patches
+
+    def _mask_patches(self, patches, p):
+        """
+        mask patches according to p
+        """
+        b, n, _ = patches.shape
+        mask = torch.rand(b, n, device=patches.device) < p
+        masked_patches = patches.clone()
+        masked_patches[mask] = self.mask_token
+        return masked_patches
+
+    def _mask_track_as_first(self, track):
+        """
+        mask out all frames to have the same token as the first frame
+        """
+        mask_track = track.clone() # b, t, n, d
+        mask_track[:, 1:] = track[:, [0]]
+        return mask_track
+
+    def forward(self, vid, track, task_emb, p_img):
+        assert torch.max(vid) <= 1.
+        B, T, C, H, W = vid.shape
+        patches = self._encode_video(vid, p_img)  # (b, n_image, d)
+        
+        if track is None:
+            # Create a dummy track if track is None
+            track = torch.zeros((B, self.num_track_ts, self.num_track_ids, 2), device=vid.device)
+        
+        enc_track = self._encode_track(track)
+    
+        text_encoded = self.language_encoder(task_emb)  # (b, c)
+        text_encoded = rearrange(text_encoded, 'b c -> b 1 c')
+    
+        x = torch.cat([enc_track, patches, text_encoded], dim=1)
+        # print(f"Shape before transformer: {x.shape}")
+        x = self.transformer(x)
+        # print(f"Shape after transformer: {x.shape}")
+    
+        # Extract the CLS token
+        cls_token = x[:, 0]
+    
+        # Get the track representation
+        track_rep = x[:, 1:self.num_track_patches+1]
+    
+        rec_patches = self.img_decoder(x[:, self.num_track_patches+1:-1])
+        # print(f"rec_patches shape: {rec_patches.shape}")
+        # print(f"patches shape: {patches.shape}")
+    
+        num_track_h = self.num_track_ts // self.track_patch_size
+        rec_track = self.track_decoder(track_rep)
+        rec_track = rearrange(rec_track, 'b (t n) (p c) -> b (t p) n c', p=self.track_patch_size, t=num_track_h)
+    
+        return rec_track, rec_patches, cls_token, track_rep
+
+    def reconstruct(self, vid, track, task_emb, p_img):
+        assert len(vid.shape) == 5  # b, t, c, h, w
+        track = self._preprocess_track(track)
+        vid = self._preprocess_vid(vid)
+        return self.forward(vid, track, task_emb, p_img)
+
+    def forward_loss(self,
+                     vid,
+                     track,
+                     task_emb,
+                     lbd_track,
+                     lbd_img,
+                     p_img,
+                     return_outs=False,
+                     vis=None):
+        """
+        track: (b, tl, n, 2), which means current time step t0 -> t0 + tl
+        vid: (b, t, c, h, w), which means the past time step t0 - t -> t0
+        task_emb: (b, e)
+        """
+
+        b, tl, n, _ = track.shape
+        if vis is None:
+            vis = torch.ones((b, tl, n)).to(track.device)
+
+        track = self._preprocess_track(track)
+        vid = self._preprocess_vid(vid)
+        vis = self._preprocess_vis(vis)
+
+        rec_track, rec_patches = self.forward(vid, track, task_emb, p_img)
+        vis[vis == 0] = .1
+        vis = repeat(vis, "b tl n -> b tl n c", c=2)
+
+        track_loss = torch.mean((rec_track - track) ** 2 * vis)
+        img_loss = torch.mean((rec_patches - self._patchify(vid)) ** 2)
+        loss = lbd_track * track_loss + lbd_img * img_loss
+
+        ret_dict = {
+            "loss": loss.item(),
+            "track_loss": track_loss.item(),
+            "img_loss": img_loss.item(),
+        }
+
+        if return_outs:
+            return loss.sum(), ret_dict, (rec_track, rec_patches)
+        return loss.sum(), ret_dict
+
+    def forward_vis(self, vid, track, task_emb, p_img):
+        b = vid.shape[0]
+        assert b == 1, "only support batch size 1 for visualization"
+    
+        H, W = self.img_size
+        _vid = vid.clone()
+        track = self._preprocess_track(track)
+        vid = self._preprocess_vid(vid)
+    
+        rec_track, rec_patches, cls_token, track_rep = self.forward(vid, track, task_emb, p_img)
+    
+        patchified_vid = self._patchify(vid)
+        # print(f"rec_patches shape: {rec_patches.shape}")
+        # print(f"patchified vid shape: {patchified_vid.shape}")
+        
+        # Ensure the shapes match for loss calculation
+        min_patches = min(rec_patches.shape[1], patchified_vid.shape[1])
+        rec_patches = rec_patches[:, :min_patches, :]
+        patchified_vid = patchified_vid[:, :min_patches, :]
+        
+        track_loss = F.mse_loss(rec_track, track)
+        img_loss = F.mse_loss(rec_patches, patchified_vid)
+        loss = track_loss + img_loss
+    
+        # print(f"Before _unpatchify: rec_patches shape = {rec_patches.shape}")
+        rec_image = self._unpatchify(rec_patches)
+        # print(f"After _unpatchify: rec_image shape = {rec_image.shape}")
+
+        # place them side by side
+        combined_image = torch.cat([vid[:, -1], rec_image[:, -1]], dim=-1)  # only visualize the current frame
+        combined_image = self.img_unnormalizer(combined_image) * 255
+        combined_image = torch.clamp(combined_image, 0, 255)
+        combined_image = rearrange(combined_image, '1 c h w -> h w c')
+
+        track = track.clone()
+        rec_track = rec_track.clone()
+
+        rec_track_vid = tracks_to_video(rec_track, img_size=H)
+        track_vid = tracks_to_video(track, img_size=H)
+
+        combined_track_vid = torch.cat([track_vid, rec_track_vid], dim=-1)
+
+        _vid = torch.cat([_vid, _vid], dim=-1)
+        combined_track_vid = _vid * .25 + combined_track_vid * .75
+
+        ret_dict = {
+            "loss": loss.sum().item(),
+            "track_loss": track_loss.sum().item(),
+            "img_loss": img_loss.sum().item(),
+            "combined_image": combined_image.cpu().numpy().astype(np.uint8),
+            "combined_track_vid": combined_track_vid.cpu().numpy().astype(np.uint8),
+        }
+
+        return loss.sum(), ret_dict
+
+    def _patchify(self, imgs):
+        """
+        imgs: (N, T, 3, H, W)
+        x: (N, L, patch_size**2 * T * 3)
+        """
+        N, T, C, img_H, img_W = imgs.shape
+        p = self.img_proj_encoder.patch_size[0]
+        assert img_H % p == 0 and img_W % p == 0
+    
+        h = img_H // p
+        w = img_W // p
+        x = imgs.reshape(shape=(N, T, C, h, p, w, p))
+        x = rearrange(x, "n t c h p w q -> n (h w) (p q t c)")
+        return x
+
+    def _unpatchify(self, x):
+        """
+        x: (N, L, patch_size**2 * T * 3)
+        imgs: (N, T, 3, H, W)
+        """
+        p = self.img_proj_encoder.patch_size[0]
+        h = self.img_size[0] // p
+        w = self.img_size[1] // p
+        
+        # print(f"_unpatchify: x.shape = {x.shape}")
+        # print(f"_unpatchify: Expected patches = {h * w}, Actual patches = {x.shape[1]}")
+        
+        if h * w != x.shape[1]:
+            # print(f"Warning: Number of patches mismatch. Expected {h * w}, got {x.shape[1]}. Adjusting...")
             # Pad or trim x to match the expected number of patches
             if h * w > x.shape[1]:
                 pad_size = h * w - x.shape[1]
diff --git a/atm/policy/vilt.py b/atm/policy/vilt.py
index 8062df8..bd481cb 100644
--- a/atm/policy/vilt.py
+++ b/atm/policy/vilt.py
@@ -155,11 +155,11 @@ class BCViLTPolicy(nn.Module):
         modality_idx += [len(self.image_encoders) + 1]  # for sentence embedding
         self.modality_idx = torch.LongTensor(modality_idx)
     
-        print(f"spatial_token shape: {spatial_token.shape}")
-        print(f"img_patch_pos_embed shape: {img_patch_pos_embed.shape}")
-        print(f"track_pos_embed shape: {track_pos_embed.shape}")
-        print(f"modality_embed shape: {modality_embed.shape}")
-        print(f"modality_idx shape: {self.modality_idx.shape}")
+        # print(f"spatial_token shape: {spatial_token.shape}")
+        # print(f"img_patch_pos_embed shape: {img_patch_pos_embed.shape}")
+        # print(f"track_pos_embed shape: {track_pos_embed.shape}")
+        # print(f"modality_embed shape: {modality_embed.shape}")
+        # print(f"modality_idx shape: {self.modality_idx.shape}")
 
     def _setup_extra_state_encoder(self, **extra_state_encoder_cfg):
         if len(self.extra_state_keys) == 0:
@@ -210,7 +210,7 @@ class BCViLTPolicy(nn.Module):
 
     def _setup_policy_head(self, network_name, **policy_head_kwargs):
         policy_head_kwargs["input_size"] = self.temporal_embed_size
-        print(f"Policy head input size: {policy_head_kwargs['input_size']}")
+        # print(f"Policy head input size: {policy_head_kwargs['input_size']}")
     
         action_shape = policy_head_kwargs["output_size"]
         self.act_shape = action_shape
@@ -273,7 +273,7 @@ class BCViLTPolicy(nn.Module):
         # Concatenate cls_token and track_rep
         track_encoded = torch.cat([cls_token, track_rep], dim=-1)
         
-        print(f"track_encoded shape after _encode_track: {track_encoded.shape}")
+        # print(f"track_encoded shape after _encode_track: {track_encoded.shape}")
         
         return track_encoded
 
@@ -300,28 +300,28 @@ class BCViLTPolicy(nn.Module):
     
         # 3. encode track
         track_encoded = self._encode_track(track_obs, task_emb)  # track_encoded: (b, v, t, cls_dim + track_rep_dim)
-        print(f"track_encoded shape: {track_encoded.shape}")
+        # print(f"track_encoded shape: {track_encoded.shape}")
         
         track_encoded = rearrange(track_encoded, 'b v t c -> (b t) v c')
-        print(f"track_encoded shape after rearrange: {track_encoded.shape}")
+        # print(f"track_encoded shape after rearrange: {track_encoded.shape}")
         
         # Project track_encoded to match the embedding size
         track_encoded = self.track_proj1(track_encoded)
-        print(f"track_encoded shape after projection: {track_encoded.shape}")
+        # print(f"track_encoded shape after projection: {track_encoded.shape}")
         
         # Reshape back to (b, t, v, c)
         track_encoded = rearrange(track_encoded, '(b t) v c -> b t v c', b=B, t=T)
         
         # Adjust track_pos_embed to match the dimensions of track_encoded
         track_pos_embed = self.track_pos_embed.expand(B, T, self.num_views, -1)
-        print(f"track_pos_embed shape: {track_pos_embed.shape}")
+        # print(f"track_pos_embed shape: {track_pos_embed.shape}")
         
         track_encoded += track_pos_embed  # (b, t, v, c)
         
         # Flatten the view dimension
         track_encoded = rearrange(track_encoded, 'b t v c -> b t (v c)')
         track_encoded = self.track_proj2(track_encoded)
-        print(f"final track_encoded shape: {track_encoded.shape}")
+        # print(f"final track_encoded shape: {track_encoded.shape}")
     
         # 3. concat img + track + text embs then add modality embeddings
         if self.spatial_transformer_use_text:
@@ -392,7 +392,541 @@ class BCViLTPolicy(nn.Module):
         """
         x = self.spatial_encode(obs, track_obs, task_emb, extra_states)  # x: (b, t, 2+num_extra, c)
         x = self.temporal_encode(x)  # (b, t, c)
-        print(f"Shape of x before policy head: {x.shape}")
+        # print(f"Shape of x before policy head: {x.shape}")
+        dist = self.policy_head(x)  # only use the current timestep feature to predict action
+        return dist
+        
+    def forward_loss(self, obs, track_obs, track, task_emb, extra_states, action):
+        """
+        Args:
+            obs: b v t c h w
+            track_obs: b v t tt_fs c h w
+            track: b v t track_len n 2, not used for training, only preserved for unified interface
+            task_emb: b emb_size
+            action: b t act_dim
+        """
+        obs, track, action = self.preprocess(obs, track, action)
+        dist = self.forward(obs, track_obs, track, task_emb, extra_states)
+        loss = self.policy_head.loss_fn(dist, action, reduction="mean")
+
+        ret_dict = {
+            "bc_loss": loss.sum().item(),
+        }
+
+        if not self.policy_head.deterministic:
+            # pseudo loss
+            sampled_action = dist.sample().detach()
+            mse_loss = F.mse_loss(sampled_action, action)
+            ret_dict["pseudo_sampled_action_mse_loss"] = mse_loss.sum().item()
+
+        ret_dict["loss"] = ret_dict["bc_loss"]
+        return loss.sum(), ret_dict
+
+    def forward_vis(self, obs, track_obs, track, task_emb, extra_states, action):
+        """
+        Args:
+            obs: b v t c h w
+            track_obs: b v t tt_fs c h w
+            track: b v t track_len n 2
+            task_emb: b emb_size
+        Returns:
+        """
+        _, track, _ = self.preprocess(obs, track, action)
+        track = track[:, :, 0, :, :, :]  # (b, v, track_len, n, 2) use the track in the first timestep
+
+        b, v, t, track_obs_t, c, h, w = track_obs.shape
+        if t >= self.num_track_ts:
+            track_obs = track_obs[:, :, :self.num_track_ts, ...]
+            track = track[:, :, :self.num_track_ts, ...]
+        else:
+            last_obs = track_obs[:, :, -1:, ...]
+            pad_obs = repeat(last_obs, "b v 1 track_obs_t c h w -> b v t track_obs_t c h w", t=self.num_track_ts-t)
+            track_obs = torch.cat([track_obs, pad_obs], dim=2)
+            last_track = track[:, :, -1:, ...]
+            pad_track = repeat(last_track, "b v 1 n d -> b v tl n d", tl=self.num_track_ts-t)
+            track = torch.cat([track, pad_track], dim=2)
+
+        grid_points = sample_double_grid(4, device=track_obs.device, dtype=track_obs.dtype)
+        grid_track = repeat(grid_points, "n d -> b v tl n d", b=b, v=v, tl=self.num_track_ts)
+
+        all_ret_dict = {}
+        for view in range(self.num_views):
+            gt_track = track[:1, view]  # (1 tl n d)
+            gt_track_vid = tracks_to_video(gt_track, img_size=h)
+            combined_gt_track_vid = (track_obs[:1, view, 0, :, ...] * .25 + gt_track_vid * .75).cpu().numpy().astype(np.uint8)
+
+            _, ret_dict = self.track.forward_vis(track_obs[:1, view, 0, :, ...], grid_track[:1, view], task_emb[:1], p_img=0)
+            ret_dict["combined_track_vid"] = np.concatenate([combined_gt_track_vid, ret_dict["combined_track_vid"]], axis=-1)
+
+            all_ret_dict = {k: all_ret_dict.get(k, []) + [v] for k, v in ret_dict.items()}
+
+        for k, v in all_ret_dict.items():
+            if k == "combined_image" or k == "combined_track_vid":
+                all_ret_dict[k] = np.concatenate(v, axis=-2)  # concat on the height dimension
+            else:
+                all_ret_dict[k] = np.mean(v)
+        return None, all_ret_dict
+
+    def act(self, obs, task_emb, extra_states):
+        """
+        Args:
+            obs: (b, v, h, w, c)
+            task_emb: (b, em_dim)
+            extra_states: {k: (b, state_dim,)}
+        """
+        self.eval()
+        B = obs.shape[0]
+
+        # expand time dimenstion
+        obs = rearrange(obs, "b v h w c -> b v 1 c h w").copy()
+        extra_states = {k: rearrange(v, "b e -> b 1 e") for k, v in extra_states.items()}
+
+        dtype = next(self.parameters()).dtype
+        device = next(self.parameters()).device
+        obs = torch.Tensor(obs).to(device=device, dtype=dtype)
+        task_emb = torch.Tensor(task_emb).to(device=device, dtype=dtype)
+        extra_states = {k: torch.Tensor(v).to(device=device, dtype=dtype) for k, v in extra_states.items()}
+
+        if (obs.shape[-2] != self.obs_shapes["rgb"][-2]) or (obs.shape[-1] != self.obs_shapes["rgb"][-1]):
+            obs = rearrange(obs, "b v fs c h w -> (b v fs) c h w")
+            obs = F.interpolate(obs, size=self.obs_shapes["rgb"][-2:], mode="bilinear", align_corners=False)
+            obs = rearrange(obs, "(b v fs) c h w -> b v fs c h w", b=B, v=self.num_views)
+
+        while len(self.track_obs_queue) < self.max_seq_len:
+            self.track_obs_queue.append(torch.zeros_like(obs))
+        self.track_obs_queue.append(obs.clone())
+        track_obs = torch.cat(list(self.track_obs_queue), dim=2)  # b v fs c h w
+        track_obs = rearrange(track_obs, "b v fs c h w -> b v 1 fs c h w")
+
+        obs = self._preprocess_rgb(obs)
+
+        with torch.no_grad():
+            x, rec_tracks = self.spatial_encode(obs, track_obs, task_emb=task_emb, extra_states=extra_states, return_recon=True)  # x: (b, 1, 4, c), recon_track: (b, v, 1, tl, n, 2)
+            self.latent_queue.append(x)
+            x = torch.cat(list(self.latent_queue), dim=1)  # (b, t, 4, c)
+            x = self.temporal_encode(x)  # (b, t, c)
+
+            feat = torch.cat([x[:, -1], rearrange(rec_tracks[:, :, -1, :, :, :], "b v tl n d -> b (v tl n d)")], dim=-1)
+
+            action = self.policy_head.get_action(feat)  # only use the current timestep feature to predict action
+            action = action.detach().cpu()  # (b, act_dim)
+
+        action = action.reshape(-1, *self.act_shape)
+        action = torch.clamp(action, -1, 1)
+        return action.float().cpu().numpy(), (None, rec_tracks[:, :, -1, :, :, :])  # (b, *act_shape)
+
+    def reset(self):
+        self.latent_queue.clear()
+        self.track_obs_queue.clear()
+
+    def save(self, path):
+        torch.save(self.state_dict(), path)
+
+    def load(self, path):
+        self.load_state_dict(torch.load(path, map_location="cpu"))
+
+    def train(self, mode=True):
+        super().train(mode)
+        self.track.eval()
+
+    def eval(self):
+        super().eval()
+        self.track.eval()
+import numpy as np
+from collections import deque
+import robomimic.utils.tensor_utils as TensorUtils
+from omegaconf import OmegaConf
+import torch
+import torch.nn as nn
+import torchvision.transforms as T
+
+from einops import rearrange, repeat
+
+from atm.model import *
+from atm.model.track_patch_embed import TrackPatchEmbed
+from atm.policy.vilt_modules.transformer_modules import *
+from atm.policy.vilt_modules.rgb_modules import *
+from atm.policy.vilt_modules.language_modules import *
+from atm.policy.vilt_modules.extra_state_modules import ExtraModalityTokens
+from atm.policy.vilt_modules.policy_head import *
+from atm.utils.flow_utils import ImageUnNormalize, sample_double_grid, tracks_to_video
+
+###############################################################################
+#
+# A ViLT Policy
+#
+###############################################################################
+
+
+class BCViLTPolicy(nn.Module):
+    """
+    Input: (o_{t-H}, ... , o_t)
+    Output: a_t or distribution of a_t
+    """
+
+    def __init__(self, obs_cfg, img_encoder_cfg, language_encoder_cfg, extra_state_encoder_cfg, track_cfg,
+                 spatial_transformer_cfg, temporal_transformer_cfg,
+                 policy_head_cfg, load_path=None):
+        super().__init__()
+
+        self._process_obs_shapes(**obs_cfg)
+
+        # 1. encode image
+        self._setup_image_encoder(**img_encoder_cfg)
+
+        # 2. encode language (spatial)
+        self.language_encoder_spatial = self._setup_language_encoder(output_size=self.spatial_embed_size, **language_encoder_cfg)
+
+        # 3. Track Transformer module
+        self._setup_track(**track_cfg)
+
+        # 3. define spatial positional embeddings, modality embeddings, and spatial token for summary
+        self._setup_spatial_positional_embeddings()
+
+        # 4. define spatial transformer
+        self._setup_spatial_transformer(**spatial_transformer_cfg)
+
+        # 5. encode extra information (e.g. gripper, joint_state)
+        self.extra_encoder = self._setup_extra_state_encoder(extra_embedding_size=self.temporal_embed_size, **extra_state_encoder_cfg)
+
+        # 6. encode language (temporal), this will also act as the TEMPORAL_TOKEN, i.e., CLS token for action prediction
+        self.language_encoder_temporal = self._setup_language_encoder(output_size=self.temporal_embed_size, **language_encoder_cfg)
+
+        # 7. define temporal transformer
+        self._setup_temporal_transformer(**temporal_transformer_cfg)
+
+        # 8. define policy head
+        self._setup_policy_head(**policy_head_cfg)
+
+        self.track_proj1 = nn.Linear(49536, self.spatial_embed_size)  # Adjust input size as needed
+        self.track_proj2 = nn.Linear(256, self.spatial_embed_size)
+    
+
+        if load_path is not None:
+            self.load(load_path)
+            self.track.load(f"{track_cfg.track_fn}/model_best.ckpt")
+
+    def _process_obs_shapes(self, obs_shapes, num_views, extra_states, img_mean, img_std, max_seq_len):
+        self.img_normalizer = T.Normalize(img_mean, img_std)
+        self.img_unnormalizer = ImageUnNormalize(img_mean, img_std)
+        self.obs_shapes = obs_shapes
+        self.policy_num_track_ts = obs_shapes["tracks"][0]
+        self.policy_num_track_ids = obs_shapes["tracks"][1]
+        self.num_views = num_views
+        self.extra_state_keys = extra_states
+        self.max_seq_len = max_seq_len
+        # define buffer queue for encoded latent features
+        self.latent_queue = deque(maxlen=max_seq_len)
+        self.track_obs_queue = deque(maxlen=max_seq_len)
+
+    def _setup_image_encoder(self, network_name, patch_size, embed_size, no_patch_embed_bias):
+        self.spatial_embed_size = embed_size
+        self.image_encoders = []
+        for _ in range(self.num_views):
+            input_shape = self.obs_shapes["rgb"]
+            self.image_encoders.append(eval(network_name)(input_shape=input_shape, patch_size=patch_size,
+                                                          embed_size=self.spatial_embed_size,
+                                                          no_patch_embed_bias=no_patch_embed_bias))
+        self.image_encoders = nn.ModuleList(self.image_encoders)
+
+        self.img_num_patches = sum([x.num_patches for x in self.image_encoders])
+
+    def _setup_language_encoder(self, network_name, **language_encoder_kwargs):
+        return eval(network_name)(**language_encoder_kwargs)
+
+    def _setup_track(self, track_fn, policy_track_patch_size=None, use_zero_track=False):
+        """
+        track_fn: path to the track model
+        policy_track_patch_size: The patch size of TrackPatchEmbedding in the policy, if None, it will be assigned the same patch size as TrackTransformer by default
+        use_zero_track: whether to zero out the tracks (ie use only the image)
+        """
+        track_cfg = OmegaConf.load(f"{track_fn}/config.yaml")
+        self.use_zero_track = use_zero_track
+
+        track_cfg.model_cfg.load_path = f"{track_fn}/model_best.ckpt"
+        track_cls = eval(track_cfg.model_name)
+        self.track = track_cls(**track_cfg.model_cfg)
+        # freeze
+        self.track.eval()
+        for param in self.track.parameters():
+            param.requires_grad = False
+
+        self.num_track_ids = self.track.num_track_ids
+        self.num_track_ts = self.track.num_track_ts
+        self.policy_track_patch_size = self.track.track_patch_size if policy_track_patch_size is None else policy_track_patch_size
+
+
+        self.track_proj_encoder = TrackPatchEmbed(
+            num_track_ts=self.policy_num_track_ts,
+            num_track_ids=self.num_track_ids,
+            patch_size=self.policy_track_patch_size,
+            in_dim=2 + self.num_views,  # X, Y, one-hot view embedding
+            embed_dim=self.spatial_embed_size)
+
+        self.track_id_embed_dim = 16
+        self.num_track_patches_per_view = self.track_proj_encoder.num_patches_per_track
+        self.num_track_patches = self.num_track_patches_per_view * self.num_views
+
+    def _setup_spatial_positional_embeddings(self):
+        # setup positional embeddings
+        spatial_token = nn.Parameter(torch.randn(1, 1, self.spatial_embed_size))  # SPATIAL_TOKEN
+        img_patch_pos_embed = nn.Parameter(torch.randn(1, self.img_num_patches, self.spatial_embed_size))
+        track_pos_embed = nn.Parameter(torch.randn(1, 1, 1, self.spatial_embed_size))  # for track embedding
+        modality_embed = nn.Parameter(
+            torch.randn(1, len(self.image_encoders) + 1 + 1, self.spatial_embed_size)
+        )  # IMG_PATCH_TOKENS + TRACK_TOKEN + SENTENCE_TOKEN
+    
+        self.register_parameter("spatial_token", spatial_token)
+        self.register_parameter("img_patch_pos_embed", img_patch_pos_embed)
+        self.register_parameter("track_pos_embed", track_pos_embed)
+        self.register_parameter("modality_embed", modality_embed)
+    
+        # for selecting modality embed
+        modality_idx = []
+        for i, encoder in enumerate(self.image_encoders):
+            modality_idx += [i] * encoder.num_patches
+        modality_idx += [len(self.image_encoders)]  # for track embedding
+        modality_idx += [len(self.image_encoders) + 1]  # for sentence embedding
+        self.modality_idx = torch.LongTensor(modality_idx)
+    
+        # print(f"spatial_token shape: {spatial_token.shape}")
+        # print(f"img_patch_pos_embed shape: {img_patch_pos_embed.shape}")
+        # print(f"track_pos_embed shape: {track_pos_embed.shape}")
+        # print(f"modality_embed shape: {modality_embed.shape}")
+        # print(f"modality_idx shape: {self.modality_idx.shape}")
+
+    def _setup_extra_state_encoder(self, **extra_state_encoder_cfg):
+        if len(self.extra_state_keys) == 0:
+            return None
+        else:
+            return ExtraModalityTokens(
+                use_joint=("joint_states" in self.extra_state_keys),
+                use_gripper=("gripper_states" in self.extra_state_keys),
+                use_ee=("ee_states" in self.extra_state_keys),
+                **extra_state_encoder_cfg
+            )
+
+    def _setup_spatial_transformer(self, num_layers, num_heads, head_output_size, mlp_hidden_size, dropout,
+                                   spatial_downsample, spatial_downsample_embed_size, use_language_token=True):
+        self.spatial_transformer = TransformerDecoder(
+            input_size=self.spatial_embed_size,
+            num_layers=num_layers,
+            num_heads=num_heads,
+            head_output_size=head_output_size,
+            mlp_hidden_size=mlp_hidden_size,
+            dropout=dropout,
+        )
+
+        if spatial_downsample:
+            self.temporal_embed_size = spatial_downsample_embed_size
+            self.spatial_downsample = nn.Linear(self.spatial_embed_size, self.temporal_embed_size)
+        else:
+            self.temporal_embed_size = self.spatial_embed_size
+            self.spatial_downsample = nn.Identity()
+
+        self.spatial_transformer_use_text = use_language_token
+
+    def _setup_temporal_transformer(self, num_layers, num_heads, head_output_size, mlp_hidden_size, dropout, use_language_token=True):
+        self.temporal_position_encoding_fn = SinusoidalPositionEncoding(input_size=self.temporal_embed_size)
+
+        self.temporal_transformer = TransformerDecoder(
+            input_size=self.temporal_embed_size,
+            num_layers=num_layers,
+            num_heads=num_heads,
+            head_output_size=head_output_size,
+            mlp_hidden_size=mlp_hidden_size,
+            dropout=dropout,)
+        self.temporal_transformer_use_text = use_language_token
+
+        action_cls_token = nn.Parameter(torch.zeros(1, 1, self.temporal_embed_size))
+        nn.init.normal_(action_cls_token, std=1e-6)
+        self.register_parameter("action_cls_token", action_cls_token)
+
+    def _setup_policy_head(self, network_name, **policy_head_kwargs):
+        policy_head_kwargs["input_size"] = self.temporal_embed_size
+        # print(f"Policy head input size: {policy_head_kwargs['input_size']}")
+    
+        action_shape = policy_head_kwargs["output_size"]
+        self.act_shape = action_shape
+        self.out_shape = np.prod(action_shape)
+        policy_head_kwargs["output_size"] = self.out_shape
+        self.policy_head = eval(network_name)(**policy_head_kwargs)
+
+    @torch.no_grad()
+    def preprocess(self, obs, track, action):
+        """
+        Preprocess observations, according to an observation dictionary.
+        Return the feature and state.
+        """
+        b, v, t, c, h, w = obs.shape
+
+        action = action.reshape(b, t, self.out_shape)
+
+        obs = self._preprocess_rgb(obs)
+
+        return obs, track, action
+
+    @torch.no_grad()
+    def _preprocess_rgb(self, rgb):
+        rgb = self.img_normalizer(rgb / 255.)
+        return rgb
+
+    def _get_view_one_hot(self, tr):
+        """ tr: b, v, t, tl, n, d -> (b, v, t), tl n, d + v"""
+        b, v, t, tl, n, d = tr.shape
+        tr = rearrange(tr, "b v t tl n d -> (b t tl n) v d")
+        one_hot = torch.eye(v, device=tr.device, dtype=tr.dtype)[None, :, :].repeat(tr.shape[0], 1, 1)
+        tr_view = torch.cat([tr, one_hot], dim=-1)  # (b t tl n) v (d + v)
+        tr_view = rearrange(tr_view, "(b t tl n) v c -> b v t tl n c", b=b, v=v, t=t, tl=tl, n=n, c=d + v)
+        return tr_view
+
+    def _encode_track(self, track_obs, task_emb):
+        """
+        track_obs: b v t tt_fs c h w
+        task_emb: b e
+        Returns: b v t d
+        """
+        b, v, t, tt_fs, c, h, w = track_obs.shape
+        track_obs = rearrange(track_obs, 'b v t tt_fs c h w -> (b v t) tt_fs c h w')
+        
+        # Normalize the track_obs
+        track_obs = track_obs / 255.0  # Assuming the input is in the range [0, 255]
+        track_obs = self.img_normalizer(track_obs)
+        
+        expand_task_emb = repeat(task_emb, 'b e -> (b v t) e', v=v, t=t)
+        
+        # Create a dummy track of the correct shape
+        dummy_track = torch.zeros((b*v*t, self.track.num_track_ts, self.track.num_track_ids, 2), device=track_obs.device)
+        
+        with torch.no_grad():
+            _, _, cls_token, track_rep = self.track(track_obs, dummy_track, expand_task_emb, p_img=0)
+        
+        cls_token = rearrange(cls_token, '(b v t) d -> b v t d', b=b, v=v, t=t)
+        track_rep = rearrange(track_rep, '(b v t) n d -> b v t (n d)', b=b, v=v, t=t)
+        
+        # Concatenate cls_token and track_rep
+        track_encoded = torch.cat([cls_token, track_rep], dim=-1)
+        
+        # print(f"track_encoded shape after _encode_track: {track_encoded.shape}")
+        
+        return track_encoded
+
+    def spatial_encode(self, obs, track_obs, task_emb, extra_states, return_recon=False):
+        # 1. encode image
+        img_encoded = []
+        for view_idx in range(self.num_views):
+            img_encoded.append(
+                rearrange(
+                    TensorUtils.time_distributed(
+                        obs[:, view_idx, ...], self.image_encoders[view_idx]
+                    ),
+                    "b t c h w -> b t (h w) c",
+                )
+            )  # (b, t, num_patches, c)
+    
+        img_encoded = torch.cat(img_encoded, -2)  # (b, t, 2*num_patches, c)
+        img_encoded += self.img_patch_pos_embed.unsqueeze(0)  # (b, t, 2*num_patches, c)
+        B, T = img_encoded.shape[:2]
+    
+        # 2. encode task_emb
+        text_encoded = self.language_encoder_spatial(task_emb)  # (b, c)
+        text_encoded = text_encoded.view(B, 1, 1, -1).expand(-1, T, -1, -1)  # (b, t, 1, c)
+    
+        # 3. encode track
+        track_encoded = self._encode_track(track_obs, task_emb)  # track_encoded: (b, v, t, cls_dim + track_rep_dim)
+        # print(f"track_encoded shape: {track_encoded.shape}")
+        
+        track_encoded = rearrange(track_encoded, 'b v t c -> (b t) v c')
+        # print(f"track_encoded shape after rearrange: {track_encoded.shape}")
+        
+        # Project track_encoded to match the embedding size
+        track_encoded = self.track_proj1(track_encoded)
+        # print(f"track_encoded shape after projection: {track_encoded.shape}")
+        
+        # Reshape back to (b, t, v, c)
+        track_encoded = rearrange(track_encoded, '(b t) v c -> b t v c', b=B, t=T)
+        
+        # Adjust track_pos_embed to match the dimensions of track_encoded
+        track_pos_embed = self.track_pos_embed.expand(B, T, self.num_views, -1)
+        # print(f"track_pos_embed shape: {track_pos_embed.shape}")
+        
+        track_encoded += track_pos_embed  # (b, t, v, c)
+        
+        # Flatten the view dimension
+        track_encoded = rearrange(track_encoded, 'b t v c -> b t (v c)')
+        track_encoded = self.track_proj2(track_encoded)
+        # print(f"final track_encoded shape: {track_encoded.shape}")
+    
+        # 3. concat img + track + text embs then add modality embeddings
+        if self.spatial_transformer_use_text:
+            img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2), text_encoded], -2)  # (b, t, 2*num_img_patch + 1 + 1, c)
+            img_track_text_encoded += self.modality_embed[None, :, self.modality_idx, :]
+        else:
+            img_track_text_encoded = torch.cat([img_encoded, track_encoded.unsqueeze(2)], -2)  # (b, t, 2*num_img_patch + 1, c)
+            img_track_text_encoded += self.modality_embed[None, :, self.modality_idx[:-1], :]
+    
+        # 4. add spatial token
+        spatial_token = self.spatial_token.unsqueeze(0).expand(B, T, -1, -1)  # (b, t, 1, c)
+        encoded = torch.cat([spatial_token, img_track_text_encoded], -2)  # (b, t, 1 + 2*num_img_patch + 1 + 1, c)
+    
+        # 5. pass through transformer
+        encoded = rearrange(encoded, "b t n c -> (b t) n c")  # (b*t, 1 + 2*num_img_patch + v + 1, c)
+        out = self.spatial_transformer(encoded)
+        out = out[:, 0]  # extract spatial token as summary at o_t
+        out = self.spatial_downsample(out).view(B, T, 1, -1)  # (b, t, 1, c')
+    
+        # 6. encode extra states
+        if self.extra_encoder is None:
+            extra = None
+        else:
+            extra = self.extra_encoder(extra_states)  # (B, T, num_extra, c')
+    
+        # 7. encode language, treat it as action token
+        text_encoded_ = self.language_encoder_temporal(task_emb)  # (b, c')
+        text_encoded_ = text_encoded_.view(B, 1, 1, -1).expand(-1, T, -1, -1)  # (b, t, 1, c')
+        action_cls_token = self.action_cls_token.unsqueeze(0).expand(B, T, -1, -1)  # (b, t, 1, c')
+        if self.temporal_transformer_use_text:
+            out_seq = [action_cls_token, text_encoded_, out]
+        else:
+            out_seq = [action_cls_token, out]
+    
+        if self.extra_encoder is not None:
+            out_seq.append(extra)
+        output = torch.cat(out_seq, -2)  # (b, t, 2 or 3 + num_extra, c')
+    
+        if return_recon:
+            output = (output, None)  # We're not using reconstructed tracks anymore
+    
+        return output
+
+    def temporal_encode(self, x):
+        """
+        Args:
+            x: b, t, num_modality, c
+        Returns:
+        """
+        pos_emb = self.temporal_position_encoding_fn(x)  # (t, c)
+        x = x + pos_emb.unsqueeze(1)  # (b, t, 2+num_extra, c)
+        sh = x.shape
+        self.temporal_transformer.compute_mask(x.shape)
+
+        x = TensorUtils.join_dimensions(x, 1, 2)  # (b, t*num_modality, c)
+        x = self.temporal_transformer(x)
+        x = x.reshape(*sh)  # (b, t, num_modality, c)
+        return x[:, :, 0]  # (b, t, c)
+
+    def forward(self, obs, track_obs, track, task_emb, extra_states):
+        """
+        Return feature and info.
+        Args:
+            obs: b v t c h w
+            track_obs: b v t tt_fs c h w
+            track: not used
+            extra_states: {k: b t e}
+        """
+        x = self.spatial_encode(obs, track_obs, task_emb, extra_states)  # x: (b, t, 2+num_extra, c)
+        x = self.temporal_encode(x)  # (b, t, c)
+        # print(f"Shape of x before policy head: {x.shape}")
         dist = self.policy_head(x)  # only use the current timestep feature to predict action
         return dist
         
diff --git a/atm/policy/vilt_modules/policy_head.py b/atm/policy/vilt_modules/policy_head.py
index da6bd66..2df2def 100644
--- a/atm/policy/vilt_modules/policy_head.py
+++ b/atm/policy/vilt_modules/policy_head.py
@@ -32,7 +32,7 @@ class DeterministicHead(nn.Module):
         self.loss_coef = loss_coef
 
     def forward(self, x):
-        print(f"Shape of x in DeterministicHead: {x.shape}")
+        # print(f"Shape of x in DeterministicHead: {x.shape}")
         y = self.net(x)
         return y
 
diff --git a/atm/utils/flow_utils.py b/atm/utils/flow_utils.py
index c152517..8ac7327 100644
--- a/atm/utils/flow_utils.py
+++ b/atm/utils/flow_utils.py
@@ -275,6 +275,311 @@ def draw_traj_on_images(tracks: torch.Tensor, images: np.ndarray, show_dots=Fals
     return result_images
 
 
+def sample_from_mask(mask, num_samples=16, replace=False):
+    """
+    mask: (H, W, 1) np
+    num_samples: int, number of samples to take
+    return: (num_samples, 2), where this is the (u, v) coordinates of the sampled pixels in the mask
+    """
+
+    # write the code according to the docstring above
+    h, w, c = mask.shape
+    mask = rearrange(mask, 'h w c -> (h w) c')
+
+    idxs = np.where(mask == 255)[0]
+    if len(idxs) == 0:
+        # return random samples from the image
+        idxs = np.arange(h*w)
+        np.random.shuffle(idxs)
+
+    if num_samples == -1:
+        num_samples = len(idxs)
+    if not replace:
+        num_samples = min(num_samples, len(idxs))
+    idxs = np.random.choice(idxs, num_samples, replace=replace)
+
+    # split into x and y
+    u = idxs % w
+    v = idxs // w
+
+    return np.stack([u, v], axis=-1)
+import numpy as np
+import torch
+import torch.nn.functional as F
+from einops import rearrange, repeat
+import matplotlib.pyplot as plt
+from matplotlib import cm
+import cv2
+
+
+class ImageUnNormalize(torch.nn.Module):
+    def __init__(self, mean, std):
+        super(ImageUnNormalize, self).__init__()
+        self.mean = torch.as_tensor(mean)
+        self.std = torch.as_tensor(std)
+        if self.mean.ndim == 1:
+            self.mean = self.mean.view(-1, 1, 1)
+        if self.std.ndim == 1:
+            self.std = self.std.view(-1, 1, 1)
+
+    def forward(self, tensor):
+        self.mean = self.mean.to(tensor.device)
+        self.std = self.std.to(tensor.device)
+        return tensor * self.std + self.mean
+
+
+def get_track_displacement(tracks):
+    """
+    track: (B, T, N, 2)
+    return: (B, N)
+
+    caluculate the displacement of each track by taking the magnitude of the
+    difference between each timestep, then summing over the timesteps.
+    """
+    b, t, c, n = tracks.shape
+    diff_tracks = torch.diff(tracks, dim=1)
+    mag_tracks = torch.norm(diff_tracks, dim=-1)
+    disp_tracks = torch.sum(mag_tracks, dim=1)
+    return disp_tracks
+
+
+def sample_grid(n, device="cuda", dtype=torch.float32, left=(0.1, 0.1), right=(0.9, 0.9)):
+    # sample nxn points as a grid
+    u = torch.linspace(left[0], right[0], n, device=device, dtype=dtype)
+    v = torch.linspace(left[1], right[1], n, device=device, dtype=dtype)
+    u, v = torch.meshgrid(u, v)
+    u = u.reshape(-1)
+    v = v.reshape(-1)
+    points = torch.stack([u, v], dim=-1)
+    return points
+
+
+def sample_double_grid(n, device="cuda", dtype=torch.float32,):
+    points1 = sample_grid(n, device, dtype, left=(0.05, 0.05), right=(0.85, 0.85))
+    points2 = sample_grid(n, device, dtype, left=(0.15, 0.15), right=(0.95, 0.95))
+    points = torch.cat([points1, points2], dim=0)
+    return points
+
+
+def sample_tracks_nearest_to_grids(tracks, vis, num_samples):
+    """
+    Sample the tracks whose first points are nearest to the grids
+    Args:
+        tracks: (track_len n 2)
+        vis: (track_len n)
+        num_samples: number of tracks to sample
+    Returns:
+        (track_len num_samples 2)
+    """
+    assert num_samples == 32
+    reference_grid_points = sample_double_grid(n=4, device="cpu")  # (32, 2)
+
+    first_points = tracks[0]  # (n, 2)
+    dist = torch.norm(first_points[:, None, :] - reference_grid_points[None, :, :], dim=-1)  # (n, 32)
+    nearest_idx = torch.argmin(dist, dim=0)  # (32,)
+    nearest_tracks = tracks[:, nearest_idx, :]  # (track_len, 32, 2)
+    nearest_vis = vis[:, nearest_idx]  # (track_len, 32)
+    return nearest_tracks, nearest_vis
+
+
+def sample_tracks(tracks, num_samples=16, uniform_ratio=0.25, vis=None, motion=False, h=None):
+    """
+    tracks: (T, N, 2)
+    num_samples: int, number of samples to take
+    uniform_ratio: float, ratio of samples to take uniformly vs. according to displacement
+    return: (T, num_samples, 2)
+
+    sample num_samples tracks from the tracks tensor, using both uniform sampling and sampling according
+    to the track displacement.
+    """
+
+    t, n, c = tracks.shape
+
+    if motion:
+        mask = (tracks > 0) & (tracks < h)
+        mask = mask.all(dim=-1) # if any of u, v is out of bounds, then it's false
+        mask = mask.all(dim=0) # if any of the points in the track is out of bounds, then it's false
+
+        mask = repeat(mask, 'n -> t n', t=t)
+        tracks = tracks[mask]
+        tracks = tracks.reshape(t, -1, c)
+
+        if vis is not None:
+            t, n = vis.shape
+            vis = vis[mask]
+            vis = vis.reshape(t, -1)
+
+    num_uniform = int(num_samples * uniform_ratio)
+    num_disp = num_samples - num_uniform
+
+    uniform_idx = torch.randint(0, n, (num_uniform,))
+
+    if num_disp == 0:
+        idx = uniform_idx
+    else:
+        disp = get_track_displacement(tracks[None])[0]
+        threshold = disp.min() + (disp.max() - disp.min()) * 0.1
+        disp[disp < threshold] = 0
+        disp[disp >= threshold] = 1
+        disp_idx = torch.multinomial(disp, num_disp, replacement=True)
+
+        idx = torch.cat([uniform_idx, disp_idx], dim=-1)
+
+    sampled_tracks = tracks[:, idx]
+    if vis is not None:
+        t, n = vis.shape
+        sampled_vis = vis[:, idx]
+
+        return sampled_tracks, sampled_vis
+
+    return sampled_tracks
+
+def sample_tracks_visible_first(tracks, vis, num_samples=16):
+    """
+    Only sample points which are visible on the initial frame
+    tracks: (T, N, 2)
+    vis: (T, N)
+    num_samples: int, number of samples to take
+    return: (T, num_samples, 2)
+
+    sample num_samples tracks from the tracks tensor, using both uniform sampling and sampling according
+    to the track displacement.
+    """
+    t, n, c = tracks.shape
+
+    vis_idx = torch.where(vis[0] >0)[0]
+
+    idx = torch.randint(0, len(vis_idx), (num_samples,))
+
+    sampled_tracks = tracks[:, vis_idx[idx]]
+    sampled_vis = vis[:, vis_idx[idx]]
+    return sampled_tracks, sampled_vis
+
+
+def tracks_to_binary_img(tracks, img_size):
+    """
+    tracks: (B, T, N, 2), where each track is a sequence of (u, v) coordinates; u is width, v is height
+    return: (B, T, C, H, W)
+    """
+    from einops import repeat
+    B, T, N, C = tracks.shape
+    generation_size = 128
+    H, W = generation_size, generation_size
+
+    tracks = tracks * generation_size
+    u, v = tracks[:, :, :, 0].long(), tracks[:, :, :, 1].long()
+    u = torch.clamp(u, 0, W - 1)
+    v = torch.clamp(v, 0, H - 1)
+    uv = u + v * W
+
+    img = torch.zeros(B, T, H * W).to(tracks.device)
+    img = img.scatter(2, uv, 1).view(B, T, H, W)
+
+    # img size is b x t x h x w
+    img = repeat(img, 'b t h w -> (b t) h w')[:, None, :, :]
+    import torch.nn.functional as F
+    # Generate 5x5 gaussian kernel
+    kernel = [[0.003765, 0.015019, 0.023792, 0.015019, 0.003765],
+              [0.015019, 0.059912, 0.094907, 0.059912, 0.015019],
+              [0.023792, 0.094907, 0.150342, 0.094907, 0.023792],
+              [0.015019, 0.059912, 0.094907, 0.059912, 0.015019],
+              [0.003765, 0.015019, 0.023792, 0.015019, 0.003765]]
+    kernel /= np.max(kernel)
+    kernel = torch.FloatTensor(kernel)[None, None, :, :].to(tracks.device)
+    img = F.conv2d(img, kernel, padding=2)[:, 0, :, :]
+    img = rearrange(img, '(b t) h w -> b t h w', b=B)
+    if generation_size != img_size:
+        img = F.interpolate(img, size=(img_size, img_size), mode="bicubic")
+    img = torch.clamp(img, 0, 1)
+    img = torch.where(img < 0.05, torch.tensor(0.0), img)
+
+    img = repeat(img, 'b t h w -> b t c h w', c=3)
+
+    assert torch.max(img) <= 1
+    return img
+
+
+def tracks_to_video(tracks, img_size):
+    """
+    tracks: (B, T, N, 2), where each track is a sequence of (u, v) coordinates; u is width, v is height
+    return: (B, C, H, W)
+    """
+    B, T, N, _ = tracks.shape
+    binary_vid = tracks_to_binary_img(tracks, img_size=img_size).float()  # b, t, c, h, w
+    binary_vid[:, :, 0] = binary_vid[:, :, 1]
+    binary_vid[:, :, 2] = binary_vid[:, :, 1]
+
+    # Get blue to purple cmap
+    cmap = plt.get_cmap('coolwarm')
+    cmap = cmap(1 / np.arange(T))[:T, :3][::-1]
+    binary_vid = binary_vid.clone()
+
+    for l in range(T):
+        # interpolate betweeen blue and red
+        binary_vid[:, l, 0] = binary_vid[:, l, 0] * cmap[l, 0] * 255
+        binary_vid[:, l, 1] = binary_vid[:, l, 1] * cmap[l, 1] * 255
+        binary_vid[:, l, 2] = binary_vid[:, l, 2] * cmap[l, 2] * 255
+    # Overwride from the last frame
+    track_vid = torch.sum(binary_vid, dim=1)
+    track_vid[track_vid > 255] = 255
+    return track_vid
+
+def combine_track_and_img(track: torch.Tensor, vid: np.ndarray):
+    """
+    track: [B, T, N, 2]
+    vid: [B, C, H, W]
+    return: (B, C, H, W)
+    """
+    img_size = vid.shape[-1]
+    track_video = tracks_to_video(track, img_size)  # B 3 H W
+    track_video = track_video.detach().cpu().numpy()
+    vid = vid.copy().astype(np.float32)
+    vid[track_video > 0] = track_video[track_video > 0]
+    return vid.astype(np.uint8)
+
+
+def draw_traj_on_images(tracks: torch.Tensor, images: np.ndarray, show_dots=False):
+    """
+    tracks: [B, T, N, 2]
+    images: [B, C, H, W]
+    Returns: [B, C, H, W]
+    """
+    b, c, h, w = images.shape
+    assert c == 3
+
+    images_back = images.astype(np.uint8).copy()
+    images_back = rearrange(images_back, "b c h w -> b h w c")
+    images_back = images_back.copy()
+
+    tracks[:, :, :, 0] = torch.clamp(tracks[:, :, :, 0] * h, 0, h-1)
+    tracks[:, :, :, 1] = torch.clamp(tracks[:, :, :, 1] * w, 0, w-1)
+
+    color_map = cm.get_cmap("cool")
+    linewidth = max(int(5 * h / 512), 1)
+
+    result_images = []
+    for traj_set, img in zip(tracks, images_back):
+        traj_len  = traj_set.shape[0]
+        for traj_idx in range(traj_set.shape[1]):
+            traj = traj_set[:, traj_idx]  # (T, 2)
+
+            for s in range(traj_len - 1):
+                color = np.array(color_map((s) / max(1, traj_len - 2))[:3]) * 255  # rgb
+                # print(int(traj[s, 0]), int(traj[s, 1]), int(traj[s + 1, 0]), int(traj[s + 1, 1]))
+
+                cv2.line(img, pt1=(int(traj[s, 0]), int(traj[s, 1])), pt2=(int(traj[s + 1, 0]), int(traj[s + 1, 1])),
+                    color=color,
+                    thickness=linewidth,
+                    lineType=cv2.LINE_AA)
+                if show_dots:
+                    cv2.circle(img, (traj[s, 0], traj[s, 1]), linewidth, color, -1)
+        result_images.append(img)
+
+    result_images = np.stack(result_images, dtype=np.uint8)
+    result_images = rearrange(result_images, "b h w c -> b c h w")
+    return result_images
+
+
 def sample_from_mask(mask, num_samples=16, replace=False):
     """
     mask: (H, W, 1) np
diff --git a/conf/train_bc/libero_vilt.yaml b/conf/train_bc/libero_vilt.yaml
index 28b41fa..8f2ca08 100644
--- a/conf/train_bc/libero_vilt.yaml
+++ b/conf/train_bc/libero_vilt.yaml
@@ -41,6 +41,134 @@ train_dataset: ???
 val_dataset: ???
 val_num_demos: null
 
+env_cfg:
+  env_type: libero
+  render_gpu_ids: 0
+  vec_env_num: 10
+  horizon: 600
+  env_name: []
+  task_name: []
+  env_meta_fn: []
+
+optimizer_cfg:
+  type: optim.AdamW
+  params:
+    lr: ${lr}
+    weight_decay: 1e-4
+
+scheduler_cfg:
+  type: CosineAnnealingLR
+  params:
+    T_max: ${epochs}
+    eta_min: 0.
+    last_epoch: -1
+
+model_name: BCViLTPolicy
+model_cfg:
+  load_path: null
+  obs_cfg:
+    obs_shapes:
+      rgb: [3, 128, 128]
+      tracks: [16, 32, 2]
+    img_mean: [ 0., 0., 0. ]
+    img_std: [ 1.0, 1.0, 1.0 ]
+    num_views: 2
+    extra_states: ${extra_state_keys}
+    max_seq_len: ${frame_stack}
+  img_encoder_cfg:
+    network_name: PatchEncoder
+    patch_size: [8, 8]
+    embed_size: 128
+    no_patch_embed_bias: false
+  language_encoder_cfg:
+    network_name: MLPEncoder
+    input_size: 768
+    hidden_size: 128
+    num_layers: 1
+  extra_state_encoder_cfg:
+    extra_num_layers: 0
+    extra_hidden_size: 128
+  track_cfg:
+    track_fn: ???
+    policy_track_patch_size: 16
+    use_zero_track: false
+  spatial_transformer_cfg:
+    num_layers: 7
+    num_heads: 8
+    head_output_size: 120
+    mlp_hidden_size: 256
+    dropout: 0.1
+    spatial_downsample: true
+    spatial_downsample_embed_size: 64
+    use_language_token: false
+  temporal_transformer_cfg:
+    num_layers: 4
+    num_heads: 6
+    head_output_size: 64
+    mlp_hidden_size: 256
+    dropout: 0.1
+    use_language_token: false
+  policy_head_cfg:
+    network_name: DeterministicHead
+    output_size: [7,]
+    hidden_size: 1024
+    num_layers: 2
+    loss_coef: 1.0
+    action_squash: false
+
+dataset_cfg:
+  img_size: ${img_size}
+  frame_stack: ${frame_stack}
+  num_track_ts: ${num_track_ts}
+  num_track_ids: ${num_track_ids}
+  track_obs_fs: 1
+  augment_track: false
+  extra_state_keys: ${extra_state_keys}
+  cache_all: true
+  cache_image: true
+defaults:
+  - _self_
+
+experiment: ??? # tag for wandb and log dir
+
+hydra:
+  run:
+    dir: ./results/policy/${now:%m%d}_${experiment}_${now:%H%M}_seed${seed}
+  sweep:
+    dir: ./results/policy/${now:%m%d}_${experiment}_${now:%H%M}_seed${seed}
+    subdir: ${hydra.job.num}
+
+wandb:
+  project: atm_policy
+  name: ${now:%m%d}_${experiment}_${now:%H%M}_seed${seed}_${hydra:job.num}
+  group: ${experiment}
+
+train_gpus: [0]
+
+# Training
+lr: 5e-4
+batch_size: 16
+mix_precision: false
+num_workers: 8
+val_freq: 5
+save_freq: 10
+clip_grad: 100.
+epochs: 101
+seed: 0
+dry: false
+
+img_size: 128
+frame_stack: 10
+num_track_ts: 16
+num_track_ids: 32
+extra_state_keys: ["joint_states", "gripper_states"]
+
+aug_prob: 0.9
+
+train_dataset: ???
+val_dataset: ???
+val_num_demos: null
+
 env_cfg:
   env_type: libero
   render_gpu_ids: 0
diff --git a/engine/train_bc.py b/engine/train_bc.py
index 0c9f83d..5ea4eb4 100644
--- a/engine/train_bc.py
+++ b/engine/train_bc.py
@@ -25,6 +25,7 @@ from engine.utils import rollout, merge_results
 
 @hydra.main(config_path="../conf/train_bc", version_base="1.3")
 def main(cfg: DictConfig):
+        
     # Put the import here so that running on slurm does not have import error
     work_dir = HydraConfig.get().runtime.output_dir
     setup(cfg)
@@ -51,7 +52,14 @@ def main(cfg: DictConfig):
     fabric = Fabric(accelerator="cuda", devices=list(cfg.train_gpus), precision="bf16-mixed" if cfg.mix_precision else None, strategy="deepspeed")
     fabric.launch()
 
-    None if (cfg.dry or not fabric.is_global_zero) else init_wandb(cfg)
+    if fabric.is_global_zero and not cfg.dry:
+        try:
+            init_wandb(cfg)
+            print("Wandb initialized successfully")
+        except Exception as e:
+            print(f"Error initializing wandb: {e}")
+    else:
+        print(f"Skipping wandb initialization. Is global zero: {fabric.is_global_zero}, Dry run: {cfg.dry}")
 
     model_cls = eval(cfg.model_name)
     model = model_cls(**cfg.model_cfg)
@@ -80,38 +88,64 @@ def main(cfg: DictConfig):
             model,
             train_loader,
             optimizer,
+            cfg,
             cfg.clip_grad,
             mix_precision=cfg.mix_precision,
             scheduler=scheduler,
         )
-
+    
         train_metrics["train/lr"] = optimizer.param_groups[0]["lr"]
         metric_logger.update(**train_metrics)
 
+        print(f"Is global zero: {fabric.is_global_zero}")
+        print(f"Dry run: {cfg.dry}")
+            
         if fabric.is_global_zero:
             None if cfg.dry else wandb.log(train_metrics, step=epoch)
-
-            if epoch % cfg.val_freq == 0:
-                val_metrics = evaluate(model,
-                                          val_loader,
-                                          mix_precision=cfg.mix_precision,
-                                          tag="val")
-
-                # Save best checkpoint
-                metric_logger.update(**val_metrics)
-
-                val_metrics = {**val_metrics}
-                loss_metric = val_metrics["val/loss"]
-                is_best = best_loss_logger.update_best(loss_metric, epoch)
-
-                if is_best:
-                    model.save(f"{work_dir}/model_best.ckpt")
-                    with open(f"{work_dir}/best_epoch.txt", "w") as f:
-                        f.write(
-                            "Best epoch: %d, Best %s: %.4f"
-                            % (epoch, "loss", best_loss_logger.best_loss)
-                        )
-                None if cfg.dry else wandb.log(val_metrics, step=epoch)
+    
+            # Run validation and log metrics every epoch
+            print("Starting validation...")
+            val_metrics, val_task_losses = evaluate(model,
+                                                    val_loader,
+                                                    mix_precision=cfg.mix_precision,
+                                                    tag="val")
+            print("Finished validation")
+    
+            # Save best checkpoint
+            metric_logger.update(**val_metrics)
+    
+            loss_metric = val_metrics["val/loss"]
+            is_best = best_loss_logger.update_best(loss_metric, epoch)
+    
+            if is_best:
+                model.save(f"{work_dir}/model_best.ckpt")
+                with open(f"{work_dir}/best_epoch.txt", "w") as f:
+                    f.write(
+                        "Best epoch: %d, Best %s: %.4f"
+                        % (epoch, "loss", best_loss_logger.best_loss)
+                    )
+            
+            # Log validation metrics to wandb
+            if not cfg.dry:
+                try:
+                    # Log general validation loss
+                    wandb.log({"val/loss": val_metrics["val/loss"]}, step=epoch)
+                    
+                    # Log per-task validation losses
+                    for task_name, task_loss in val_task_losses.items():
+                        wandb.log({f"val/task_loss/{task_name}": task_loss}, step=epoch)
+                    
+                    # Log best loss so far
+                    wandb.log({"val/best_loss": best_loss_logger.best_loss}, step=epoch)
+
+                    print("Successfully logged validation metrics to wandb")
+                except Exception as e:
+                    print(f"Error logging to wandb: {e}")
+    
+            print(f"Validation metrics at epoch {epoch}:")
+            print(f"General val loss: {val_metrics['val/loss']:.4f}")
+            for task_name, task_loss in val_task_losses.items():
+                print(f"Val loss for {task_name}: {task_loss:.4f}")
 
         if epoch % cfg.save_freq == 0:
             model.save(f"{work_dir}/model_{epoch}.ckpt")
@@ -151,32 +185,42 @@ def main(cfg: DictConfig):
         None if cfg.dry else wandb.finish()
 
 
-def run_one_epoch(fabric,
-                  model,
-                  dataloader,
-                  optimizer,
-                  clip_grad=1.0,
-                  mix_precision=False,
-                  scheduler=None,
-                  ):
-    """
-    Optimize the policy. Return a dictionary of the loss and any other metrics.
-    """
+def run_one_epoch(fabric, model, dataloader, optimizer, cfg, clip_grad=1.0, mix_precision=False, scheduler=None):
     tot_loss_dict, tot_items = {}, 0
+    task_losses = {task_name: [] for task_name in dataloader.dataset.task_names}
+    lang_grad_norms = []
+    total_loss = 0
+    total_bc_loss = 0
 
     model.train()
-    i = 0
-    for obs, track_obs, track, task_emb, action, extra_states in tqdm(dataloader):
+    for batch_idx, (obs, track_obs, track, task_emb, action, extra_states, task_ids) in enumerate(tqdm(dataloader)):
         if mix_precision:
             obs, track_obs, track, task_emb, action = obs.bfloat16(), track_obs.bfloat16(), track.bfloat16(), task_emb.bfloat16(), action.bfloat16()
             extra_states = {k: v.bfloat16() for k, v in extra_states.items()}
 
+        # Print tasks in this batch
+        tasks_in_batch = [dataloader.dataset.task_names[task_id.item()] for task_id in task_ids]
+        # print(f"Batch {batch_idx}, Tasks: {tasks_in_batch}")
+
+        # Ensure task_emb requires grad
+        task_emb.requires_grad_(True)
+
         loss, ret_dict = model.forward_loss(obs, track_obs, track, task_emb, extra_states, action)
         optimizer.zero_grad()
         fabric.backward(loss)
 
-        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)
+        # Calculate gradient magnitude on language input
+        lang_grad_norm = torch.norm(task_emb.grad).item() if task_emb.grad is not None else 0
+        lang_grad_norms.append(lang_grad_norm)
+
+        # Print task embedding and its gradient for the first few batches
+        # if batch_idx < 5:
+            # print(f"Batch {batch_idx}:")
+            # print(f"Task embedding shape: {task_emb.shape}")
+            # print(f"Task embedding (first sample): {task_emb[0, :10]}")
+            # print(f"Task embedding grad (first sample): {task_emb.grad[0, :10] if task_emb.grad is not None else 'None'}")
 
+        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)
         optimizer.step()
 
         for k, v in ret_dict.items():
@@ -185,7 +229,39 @@ def run_one_epoch(fabric,
             tot_loss_dict[k] += v
         tot_items += 1
 
-        i += 1
+        # Accumulate losses for each task
+        for i, task_id in enumerate(task_ids):
+            task_name = dataloader.dataset.task_names[task_id.item()]
+            task_loss = ret_dict['bc_loss'] / len(task_ids)
+            task_losses[task_name].append(task_loss)
+
+        # Accumulate total loss and bc_loss
+        total_loss += loss.item()
+        total_bc_loss += ret_dict['bc_loss']
+
+    # Calculate and log average losses for each task over the entire epoch
+    avg_task_losses = {k: sum(v) / len(v) for k, v in task_losses.items() if v}
+    
+    # Calculate average total loss and bc_loss
+    avg_total_loss = total_loss / tot_items
+    avg_bc_loss = total_bc_loss / tot_items
+
+    if fabric.is_global_zero and not cfg.dry:
+        # Log general metrics
+        wandb.log({
+            "train/epoch_avg_loss": avg_total_loss,
+            "train/epoch_avg_bc_loss": avg_bc_loss,
+        })
+        
+        # Log task-specific metrics
+        wandb.log({f"train/epoch_avg_bc_loss_{k}": v for k, v in avg_task_losses.items()})
+
+    # Log average gradient magnitude on language input for the entire epoch
+    avg_lang_grad_norm = sum(lang_grad_norms) / len(lang_grad_norms)
+    if fabric.is_global_zero:
+        if not cfg.dry:
+            wandb.log({"train/epoch_avg_lang_grad_norm": avg_lang_grad_norm})
+        print(f"Average language gradient norm: {avg_lang_grad_norm}")
 
     out_dict = {}
     for k, v in tot_loss_dict.items():
@@ -195,15 +271,14 @@ def run_one_epoch(fabric,
         scheduler.step()
 
     return out_dict
-
-
+    
 @torch.no_grad()
 def evaluate(model, dataloader, mix_precision=False, tag="val"):
     tot_loss_dict, tot_items = {}, 0
+    task_losses = {task_name: [] for task_name in dataloader.dataset.task_names}
     model.eval()
 
-    i = 0
-    for obs, track_obs, track, task_emb, action, extra_states in tqdm(dataloader):
+    for obs, track_obs, track, task_emb, action, extra_states, task_ids in tqdm(dataloader):
         obs, track_obs, track, task_emb, action = obs.cuda(), track_obs.cuda(), track.cuda(), task_emb.cuda(), action.cuda()
         extra_states = {k: v.cuda() for k, v in extra_states.items()}
         if mix_precision:
@@ -212,19 +287,27 @@ def evaluate(model, dataloader, mix_precision=False, tag="val"):
 
         _, ret_dict = model.forward_loss(obs, track_obs, track, task_emb, extra_states, action)
 
-        i += 1
-
         for k, v in ret_dict.items():
             if k not in tot_loss_dict:
                 tot_loss_dict[k] = 0
             tot_loss_dict[k] += v
         tot_items += 1
 
+        # Accumulate losses for each task
+        for i, task_id in enumerate(task_ids):
+            task_name = dataloader.dataset.task_names[task_id.item()]
+            task_loss = ret_dict['bc_loss'] / len(task_ids)
+            task_losses[task_name].append(task_loss)
+
     out_dict = {}
     for k, v in tot_loss_dict.items():
         out_dict[f"{tag}/{k}"] = tot_loss_dict[f"{k}"] / tot_items
 
-    return out_dict
+    # Calculate and add average losses for each task
+    avg_task_losses = {k: sum(v) / len(v) for k, v in task_losses.items() if v}
+    out_dict.update({f"{tag}/avg_bc_loss_{k}": v for k, v in avg_task_losses.items()})
+
+    return out_dict, avg_task_losses
 
 
 @torch.no_grad()
@@ -232,7 +315,7 @@ def visualize(model, dataloader, mix_precision=False):
     model.eval()
     keep_eval_dict = None
 
-    for obs, track_obs, track, task_emb, action, extra_states in dataloader:
+    for obs, track_obs, track, task_emb, action, extra_states, task_ids in dataloader:
         obs, track_obs, track, task_emb = obs.cuda(), track_obs.cuda(), track.cuda(), task_emb.cuda()
         extra_states = {k: v.cuda() for k, v in extra_states.items()}
         if mix_precision:
diff --git a/scripts/train_libero_policy_atm.py b/scripts/train_libero_policy_atm.py
index 6a01256..7e5c9fa 100755
--- a/scripts/train_libero_policy_atm.py
+++ b/scripts/train_libero_policy_atm.py
@@ -2,6 +2,56 @@ import os
 import argparse
 
 
+# environment variables
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+# default track transformer path
+DEFAULT_TRACK_TRANSFORMERS = {
+    "libero_spatial": "./results/track_transformer/libero_track_transformer_libero-spatial/",
+    "libero_object": "./results/track_transformer/libero_track_transformer_libero-object/",
+    "libero_goal": "./results/track_transformer/libero_track_transformer_libero-goal/",
+    "libero_10": "./results/track_transformer/libero_track_transformer_libero-100/",
+}
+
+# input parameters
+parser = argparse.ArgumentParser()
+parser.add_argument("--suite", default="libero_goal", choices=["libero_spatial", "libero_object", "libero_goal", "libero_10"], 
+                    help="The name of the desired suite, where libero_10 is the alias of libero_long.")
+parser.add_argument("-tt", "--track-transformer", default=None, help="Then path to the trained track transformer.")
+args = parser.parse_args()
+
+# training configs
+CONFIG_NAME = "libero_vilt"
+
+train_gpu_ids = [0]
+NUM_DEMOS = 10
+
+root_dir = "./data/atm_libero/"
+suite_name = args.suite
+task_dir_list = os.listdir(os.path.join(root_dir, suite_name))
+task_dir_list.sort()
+
+# dataset
+train_path_list = [f"{root_dir}/{suite_name}/{task_dir}/bc_train_{NUM_DEMOS}" for task_dir in task_dir_list]
+val_path_list = [f"{root_dir}/{suite_name}/{task_dir}/val" for task_dir in task_dir_list]
+
+track_fn = args.track_transformer or DEFAULT_TRACK_TRANSFORMERS[suite_name]
+
+for seed in range(3):
+    commond = (f'python -m engine.train_bc --config-name={CONFIG_NAME} train_gpus="{train_gpu_ids}" '
+                f'experiment=atm-policy_{suite_name.replace("_", "-")}_demo{NUM_DEMOS} '
+                f'train_dataset="{train_path_list}" val_dataset="{val_path_list}" '
+                f'model_cfg.track_cfg.track_fn={track_fn} '
+                f'model_cfg.track_cfg.use_zero_track=False '
+                f'model_cfg.spatial_transformer_cfg.use_language_token=False '
+                f'model_cfg.temporal_transformer_cfg.use_language_token=False '
+                f'seed={seed} ')
+
+    os.system(commond)
+import os
+import argparse
+
+
 # environment variables
 os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
